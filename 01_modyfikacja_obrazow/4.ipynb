{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    ToPILImage,\n",
    "    CenterCrop,\n",
    "    Resize,\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.models.vgg import vgg16\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n",
    "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
    "        self.block8 = nn.Sequential(*block8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "        block6 = self.block6(block5)\n",
    "        block7 = self.block7(block6)\n",
    "        block8 = self.block8(block1 + block7)\n",
    "\n",
    "        return (torch.tanh(block8) + 1) / 2\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class UpsampleBLock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBLock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hr_transform():\n",
    "    return Compose(\n",
    "        [\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def train_lr_transform(size):\n",
    "    return Compose(\n",
    "        [ToPILImage(), Resize(size, interpolation=Image.BICUBIC), ToTensor()]\n",
    "    )\n",
    "\n",
    "\n",
    "def display_transform():\n",
    "    return Compose([ToPILImage(), Resize(400), CenterCrop(400), ToTensor()])\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(\n",
    "        filename.endswith(extension)\n",
    "        for extension in [\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\"]\n",
    "    )\n",
    "\n",
    "\n",
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(TrainDatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = sorted([\n",
    "            os.path.join(dataset_dir, x)\n",
    "            for x in os.listdir(dataset_dir)\n",
    "            if is_image_file(x)\n",
    "        ][:100])\n",
    "        self.hr_transform = train_hr_transform()\n",
    "        self.lr_transform = train_lr_transform(256 // upscale_factor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
    "        lr_image = self.lr_transform(hr_image)\n",
    "        return lr_image, hr_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg16(pretrained=True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)[:30]).eval()\n",
    "        for param in loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TVLoss()\n",
    "\n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        # Adversarial Loss\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        # Perception Loss\n",
    "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
    "        # Image Loss\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        # TV Loss\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
    "\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 734219\n",
      "# discriminator parameters: 5215425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/5] Loss_D: 0.7280 Loss_G: 0.0359 D(x): 0.4856 D(G(z)): 0.2715: 100%|██████████| 25/25 [23:06<00:00, 55.47s/it]\n",
      "[2/5] Loss_D: 0.9946 Loss_G: 0.0241 D(x): 0.4877 D(G(z)): 0.4822: 100%|██████████| 25/25 [23:43<00:00, 56.94s/it]\n",
      "[3/5] Loss_D: 0.9940 Loss_G: 0.0215 D(x): 0.5617 D(G(z)): 0.5556: 100%|██████████| 25/25 [23:52<00:00, 57.31s/it]\n",
      "[4/5] Loss_D: 0.9913 Loss_G: 0.0191 D(x): 0.4954 D(G(z)): 0.4931: 100%|██████████| 25/25 [23:45<00:00, 57.01s/it]\n",
      "[5/5] Loss_D: 0.9968 Loss_G: 0.0185 D(x): 0.4657 D(G(z)): 0.4605: 100%|██████████| 25/25 [23:43<00:00, 56.95s/it]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"upscale_factor\": 4,\n",
    "    \"num_epochs\": 5,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "    \"dataset_path\": \"images_256\",\n",
    "}\n",
    "\n",
    "train_set = TrainDatasetFromFolder(\n",
    "    params[\"dataset_path\"], upscale_factor=params[\"upscale_factor\"]\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    num_workers=params[\"num_workers\"],\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "netG = Generator(params[\"upscale_factor\"])\n",
    "print(\"# generator parameters:\", sum(param.numel() for param in netG.parameters()))\n",
    "netD = Discriminator()\n",
    "print(\"# discriminator parameters:\", sum(param.numel() for param in netD.parameters()))\n",
    "\n",
    "generator_criterion = GeneratorLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    netG.cuda()\n",
    "    netD.cuda()\n",
    "    generator_criterion.cuda()\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "results = {\n",
    "    \"d_loss\": [],\n",
    "    \"g_loss\": [],\n",
    "    \"d_score\": [],\n",
    "    \"g_score\": [],\n",
    "}\n",
    "\n",
    "dirname = f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "os.makedirs(dirname)\n",
    "\n",
    "for epoch in range(1, params[\"num_epochs\"] + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_results = {\n",
    "        \"batch_sizes\": 0,\n",
    "        \"d_loss\": 0,\n",
    "        \"g_loss\": 0,\n",
    "        \"d_score\": 0,\n",
    "        \"g_score\": 0,\n",
    "    }\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results[\"batch_sizes\"] += batch_size\n",
    "\n",
    "        ############################\n",
    "        # (1) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "        ###########################\n",
    "        real_img = target\n",
    "        if torch.cuda.is_available():\n",
    "            real_img = real_img.float().cuda()\n",
    "        z = data\n",
    "        if torch.cuda.is_available():\n",
    "            z = z.float().cuda()\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update D network: maximize D(x)-1-D(G(z))\n",
    "        ###########################\n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img.detach()).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        d_loss.backward()\n",
    "\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "\n",
    "        optimizerD.step()\n",
    "\n",
    "        # loss for current batch before optimization\n",
    "        running_results[\"g_loss\"] += g_loss.item() * batch_size\n",
    "        running_results[\"d_loss\"] += d_loss.item() * batch_size\n",
    "        running_results[\"d_score\"] += real_out.item() * batch_size\n",
    "        running_results[\"g_score\"] += fake_out.item() * batch_size\n",
    "\n",
    "        train_bar.set_description(\n",
    "            desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\"\n",
    "            % (\n",
    "                epoch,\n",
    "                params[\"num_epochs\"],\n",
    "                running_results[\"d_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"g_loss\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"d_score\"] / running_results[\"batch_sizes\"],\n",
    "                running_results[\"g_score\"] / running_results[\"batch_sizes\"],\n",
    "            )\n",
    "        )\n",
    "            # save loss\\scores\\psnr\\ssim\n",
    "        results[\"d_loss\"].append(\n",
    "            running_results[\"d_loss\"] / running_results[\"batch_sizes\"]\n",
    "        )\n",
    "        results[\"g_loss\"].append(\n",
    "            running_results[\"g_loss\"] / running_results[\"batch_sizes\"]\n",
    "        )\n",
    "        results[\"d_score\"].append(\n",
    "            running_results[\"d_score\"] / running_results[\"batch_sizes\"]\n",
    "        )\n",
    "        results[\"g_score\"].append(\n",
    "            running_results[\"g_score\"] / running_results[\"batch_sizes\"]\n",
    "        )\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        torch.save(\n",
    "            netG.state_dict(),\n",
    "            \"%s/netG_epoch_%d_%d.pth\" % (dirname, epoch, params[\"upscale_factor\"]),\n",
    "        )\n",
    "        torch.save(\n",
    "            netD.state_dict(),\n",
    "            \"%s/netD_epoch_%d_%d.pth\" % (dirname, epoch, params[\"upscale_factor\"]),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            fake_images = netG(data[:4])\n",
    "            torchvision.utils.save_image(\n",
    "                fake_images,\n",
    "                f\"{dirname}/output_epoch_{epoch + 1}.png\",\n",
    "                normalize=True,\n",
    "            )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
