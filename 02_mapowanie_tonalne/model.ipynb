{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENCV_IO_ENABLE_OPENEXR'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.conv1(x)\n",
    "        out_1_relu = self.relu(out_1)\n",
    "\n",
    "        out_2 = self.conv2(out_1_relu)\n",
    "        out_2_relu = self.relu(out_2)\n",
    "\n",
    "        out_3 = self.conv3(out_2_relu)\n",
    "        out_3_relu = self.relu(out_3)\n",
    "\n",
    "        return out_3_relu\n",
    "\n",
    "\n",
    "input_channels = 3 \n",
    "encoder = Encoder(input_channels)\n",
    "input_tensor = torch.randn(1, input_channels, 256, 256)\n",
    "output = encoder(input_tensor)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_channels):  \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=3,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input, img_1, img_2, img_3):\n",
    "        out_1 = self.conv1(input)\n",
    "        out_1_relu = self.relu(out_1)\n",
    "        \n",
    "        out_2 = self.conv2(out_1_relu)\n",
    "        out_2_relu = self.relu(out_2)\n",
    "        \n",
    "        out_3 = self.conv3(out_2_relu)\n",
    "        \n",
    "        out = out_3 + img_1 + img_2 + img_3\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out, out_3\n",
    "\n",
    "# Przykład użycia:\n",
    "height = 256\n",
    "width = 256\n",
    "input_channels = 64  # Dopasuj do wyjścia enkodera lub swoich potrzeb\n",
    "decoder = Decoder(input_channels=input_channels)\n",
    "input_tensor = torch.randn(1, input_channels, height, width)\n",
    "img_1 = torch.randn(1, 3, height, width)\n",
    "img_2 = torch.randn(1, 3, height, width)\n",
    "img_3 = torch.randn(1, 3, height, width)\n",
    "output, out_3 = decoder(input_tensor, img_1, img_2, img_3)\n",
    "\n",
    "print(output.shape)\n",
    "print(out_3.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TMONet(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(TMONet, self).__init__()\n",
    "        \n",
    "        # Encoder - jedna instancja, współdzielona dla wszystkich wejść\n",
    "        self.encoder = Encoder(input_channels=input_channels)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.conv_gated_1 = nn.Conv2d(\n",
    "            in_channels=64 * 3,  # 64 kanały z każdego enkodera x 3\n",
    "            out_channels=64 * 3,  # sc w oryginalnym kodzie\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.conv_gated_2 = nn.Conv2d(\n",
    "            in_channels=64 * 3,\n",
    "            out_channels=64 * 3,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0  # kernel 1x1 nie wymaga paddingu dla 'SAME'\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(input_channels=64 * 3)\n",
    "\n",
    "    def forward(self, input_1, input_2, input_3):\n",
    "        # Encoder\n",
    "        output_1 = self.encoder(input_1)\n",
    "        output_2 = self.encoder(input_2)\n",
    "        output_3 = self.encoder(input_3)\n",
    "        \n",
    "        # Concatenation\n",
    "        out_concat = torch.cat([output_1, output_2, output_3], dim=1)  # dim=1 dla kanałów\n",
    "        \n",
    "        # Fusion\n",
    "        out_gated_1 = self.conv_gated_1(out_concat)\n",
    "        out_gated_2 = self.conv_gated_2(out_gated_1)\n",
    "        \n",
    "        # Decoder\n",
    "        out_img, out_res = self.decoder(out_gated_2, input_1, input_2, input_3)\n",
    "        \n",
    "        return out_img\n",
    "\n",
    "# Przykład użycia:\n",
    "tmo_net = TMONet(input_channels=3)\n",
    "height = 256\n",
    "width = 256\n",
    "input_1 = torch.randn(1, 3, height, width)\n",
    "input_2 = torch.randn(1, 3, height, width)  \n",
    "input_3 = torch.randn(1, 3, height, width)\n",
    "output = tmo_net(input_1, input_2, input_3)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.735195159912109\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def gaussian_kernel(nsig=2, filter_size=13):\n",
    "    interval = (2 * nsig + 1.0) / filter_size\n",
    "    ll = np.linspace(-nsig - interval / 2.0, nsig + interval / 2.0, filter_size + 1)\n",
    "    kern1d = np.diff(st.norm.cdf(ll))\n",
    "    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))\n",
    "    kernel = kernel_raw / kernel_raw.sum()\n",
    "    kernel = kernel.astype(np.float32)\n",
    "    return kernel  # Zwracamy tylko jądro 2D\n",
    "\n",
    "\n",
    "def feature_filtered(input_feature, sigma=2, kernel_size_num=13, kernel_size_m=3):\n",
    "    # Pobierz wymiary tensora wejściowego (batch_size, channels, height, width)\n",
    "    sb, sc, sy, sx = input_feature.shape  # PyTorch używa formatu BCHW\n",
    "\n",
    "    # Oblicz jądro Gaussa\n",
    "    kernel = gaussian_kernel(nsig=sigma, filter_size=kernel_size_num)\n",
    "    kernel = (\n",
    "        torch.from_numpy(kernel).to(input_feature.device).float()\n",
    "    )  # Konwersja na tensor\n",
    "\n",
    "    # Przygotowanie wag dla splotu głębinowego\n",
    "    weights_g = kernel.unsqueeze(0).unsqueeze(\n",
    "        0\n",
    "    )  # Dodajemy wymiary: [1, 1, height, width]\n",
    "    weights_g = weights_g.repeat(\n",
    "        sc, 1, 1, 1\n",
    "    )  # Powielamy dla każdego kanału: [sc, 1, height, width]\n",
    "\n",
    "    # Jądro prostokątne (box filter)\n",
    "    weights_m = torch.ones(\n",
    "        kernel_size_num,\n",
    "        kernel_size_num,\n",
    "        dtype=torch.float32,\n",
    "        device=input_feature.device,\n",
    "    )\n",
    "    weights_m = weights_m.unsqueeze(0).unsqueeze(0)  # [1, 1, height, width]\n",
    "    weights_m = weights_m.repeat(sc, 1, 1, 1)  # [sc, 1, height, width]\n",
    "\n",
    "    sum_k = kernel_size_num * kernel_size_num\n",
    "    sum_k = torch.tensor(sum_k, dtype=torch.float32, device=input_feature.device)\n",
    "\n",
    "    # Operacje splotu\n",
    "    ## Gaussian\n",
    "    out_gaussian = F.conv2d(\n",
    "        input_feature, weights_g, stride=1, padding=kernel_size_num // 2, groups=sc\n",
    "    )\n",
    "\n",
    "    ## Box filter\n",
    "    out_box_feature_square = F.conv2d(\n",
    "        input_feature.pow(2),\n",
    "        weights_m / sum_k,\n",
    "        stride=1,\n",
    "        padding=kernel_size_num // 2,\n",
    "        groups=sc,\n",
    "    )\n",
    "    out_box_feature_mean = F.conv2d(\n",
    "        input_feature,\n",
    "        weights_m / sum_k,\n",
    "        stride=1,\n",
    "        padding=kernel_size_num // 2,\n",
    "        groups=sc,\n",
    "    )\n",
    "    out_box_feature_mean_square = out_box_feature_mean.pow(2)\n",
    "\n",
    "    # Zabezpieczenie przed zerami w mianowniku\n",
    "    out_box_feature_mean_square = torch.where(\n",
    "        out_box_feature_mean_square == 0.0,\n",
    "        torch.full_like(out_box_feature_mean_square, 1e-8),\n",
    "        out_box_feature_mean_square,\n",
    "    )\n",
    "\n",
    "    diff = out_box_feature_square - out_box_feature_mean_square\n",
    "    local_std = torch.sqrt(torch.abs(diff) + 1e-8)\n",
    "\n",
    "    return out_gaussian, local_std, out_box_feature_mean\n",
    "\n",
    "\n",
    "def local_mean_std(input_feature, sigma=2, kernel_size_num=13, kernel_size_m=3):\n",
    "    mean_local, std_local, mean_local_box = feature_filtered(\n",
    "        input_feature,\n",
    "        sigma=sigma,\n",
    "        kernel_size_num=kernel_size_num,\n",
    "        kernel_size_m=kernel_size_m,\n",
    "    )\n",
    "\n",
    "    return mean_local, std_local, mean_local_box\n",
    "\n",
    "\n",
    "def sign_num_den(input, gamma, beta, sigma=2, kernel_size_num=13, kernel_size_m=3):\n",
    "    local_mean, local_std, local_mean_box = local_mean_std(\n",
    "        input, sigma=sigma, kernel_size_num=kernel_size_num\n",
    "    )\n",
    "\n",
    "    # Licznik (num)\n",
    "    gaussian_norm = (input - local_mean) / (torch.abs(local_mean) + 1e-8)\n",
    "    msk = torch.where(\n",
    "        gaussian_norm > 0.0,\n",
    "        torch.ones_like(gaussian_norm),\n",
    "        -torch.zeros_like(gaussian_norm),\n",
    "    )\n",
    "    gaussian_norm = torch.where(\n",
    "        gaussian_norm == 0.0, torch.full_like(gaussian_norm, 1e-8), gaussian_norm\n",
    "    )\n",
    "    gaussian_norm = torch.pow(torch.abs(gaussian_norm), gamma)\n",
    "    norm_num = msk * gaussian_norm\n",
    "\n",
    "    # Mianownik (den)\n",
    "    local_norm = local_std / (torch.abs(local_mean_box) + 1e-8)\n",
    "    norm_den = torch.pow(local_norm, beta)\n",
    "    norm_den = 1.0 + norm_den\n",
    "\n",
    "    return norm_num, norm_den\n",
    "\n",
    "\n",
    "def feature_contrast_masking(input, gamma, beta, sigma_num=2, kernel_size_num=13, kernel_size_den=13):\n",
    "    norm_num, norm_den = sign_num_den(\n",
    "        input, gamma=gamma, beta=beta, sigma=sigma_num, \n",
    "        kernel_size_num=kernel_size_num, kernel_size_m=kernel_size_den\n",
    "    )\n",
    "    out = norm_num / norm_den\n",
    "    return out\n",
    "\n",
    "def masking_loss(input_1, input_2, gamma=0.5, beta=0.5, sigma_num=2.0, kernel_size_num=13, kernel_size_den=13):\n",
    "    auto_loss_output = feature_contrast_masking(\n",
    "        input_1, gamma=1.0, beta=beta, sigma_num=sigma_num, \n",
    "        kernel_size_num=kernel_size_num, kernel_size_den=kernel_size_den\n",
    "    )\n",
    "    auto_loss_gt = feature_contrast_masking(\n",
    "        input_2, gamma=gamma, beta=beta, sigma_num=sigma_num, \n",
    "        kernel_size_num=kernel_size_num, kernel_size_den=kernel_size_den\n",
    "    )\n",
    "    diff = auto_loss_output - auto_loss_gt  # torch.subtract → bezpośrednie odejmowanie\n",
    "    diff_abs = torch.abs(diff)\n",
    "    cost = torch.mean(diff_abs)  # torch.reduce_mean → torch.mean\n",
    "    return cost\n",
    "\n",
    "\n",
    "input_1 = torch.randn(1, 3, 32, 32)  # batch_size=1, channels=3, height=32, width=32\n",
    "input_2 = torch.randn(1, 3, 32, 32)\n",
    "loss = masking_loss(input_1, input_2, gamma=0.5, beta=0.5)\n",
    "\n",
    "print(loss.item())  # Wyświetli pojedynczą wartość straty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# Funkcja pomocnicza emulująca VGG19_slim\n",
    "class VGG19FeatureExtractor(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(VGG19FeatureExtractor, self).__init__()\n",
    "        vgg19 = models.vgg19(pretrained=True).features.eval().to(device)\n",
    "        self.vgg19 = vgg19\n",
    "        self.layer_map = {\n",
    "            \"VGG11\": 0,  # conv1_1\n",
    "            \"VGG21\": 5,  # conv2_1\n",
    "            \"VGG22\": 9,  # conv2_2\n",
    "            \"VGG31\": 10,  # conv3_1\n",
    "            \"VGG34\": 18,  # conv3_4\n",
    "            \"VGG41\": 19,  # conv4_1\n",
    "            \"VGG51\": 28,  # conv5_1\n",
    "            \"VGG54\": 34,  # conv5_4\n",
    "        }\n",
    "\n",
    "    def forward(self, input, type):\n",
    "        if type not in self.layer_map:\n",
    "            raise NotImplementedError(f\"Unknown perceptual type: {type}\")\n",
    "\n",
    "        target_layer_idx = self.layer_map[type]\n",
    "        output = input\n",
    "        for idx, layer in enumerate(self.vgg19):\n",
    "            output = layer(output)\n",
    "            if idx == target_layer_idx:\n",
    "                break\n",
    "        return output\n",
    "\n",
    "\n",
    "# Funkcja FCM_loss dostosowana do VGG19_slim\n",
    "def FCM_loss(\n",
    "    input_1,\n",
    "    input_2,\n",
    "    feature_extractor,\n",
    "    gamma=0.5,\n",
    "    beta=0.5,\n",
    "    sigma_num=2,\n",
    "    kernel_size_num=13,\n",
    "    kernel_size_den=13,\n",
    "):\n",
    "    # Ensure inputs are on the correct device\n",
    "    input_1 = input_1.to(feature_extractor.vgg19[0].weight.device)\n",
    "    input_2 = input_2.to(feature_extractor.vgg19[0].weight.device)\n",
    "\n",
    "    # Extract features using the pre-loaded VGG19 model\n",
    "    x_1 = feature_extractor(input_1, \"VGG11\")\n",
    "    gt_1 = feature_extractor(input_2, \"VGG11\")\n",
    "\n",
    "    x_2 = feature_extractor(input_1, \"VGG21\")\n",
    "    gt_2 = feature_extractor(input_2, \"VGG21\")\n",
    "\n",
    "    x_3 = feature_extractor(input_1, \"VGG31\")\n",
    "    gt_3 = feature_extractor(input_2, \"VGG31\")\n",
    "\n",
    "    # Calculate costs using masking_loss\n",
    "    cost_1 = masking_loss(\n",
    "        x_1,\n",
    "        gt_1,\n",
    "        gamma=gamma,\n",
    "        beta=beta,\n",
    "        sigma_num=sigma_num,\n",
    "        kernel_size_num=kernel_size_num,\n",
    "        kernel_size_den=kernel_size_den,\n",
    "    )\n",
    "    cost_2 = masking_loss(\n",
    "        x_2,\n",
    "        gt_2,\n",
    "        gamma=gamma,\n",
    "        beta=beta,\n",
    "        sigma_num=sigma_num,\n",
    "        kernel_size_num=kernel_size_num,\n",
    "        kernel_size_den=kernel_size_den,\n",
    "    )\n",
    "    cost_3 = masking_loss(\n",
    "        x_3,\n",
    "        gt_3,\n",
    "        gamma=gamma,\n",
    "        beta=beta,\n",
    "        sigma_num=sigma_num,\n",
    "        kernel_size_num=kernel_size_num,\n",
    "        kernel_size_den=kernel_size_den,\n",
    "    )\n",
    "\n",
    "    # Average loss\n",
    "    cost_all = (cost_1 + cost_2 + cost_3) / 3.0\n",
    "    return cost_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "def mul_exp(img):\n",
    "    x_p = 1.21497  # Stała wartość\n",
    "    # Obliczanie c_start i c_end w PyTorch\n",
    "    c_start = torch.log((x_p / torch.max(img)) / torch.log(torch.tensor(2.0, device=img.device)))\n",
    "    c_end = torch.log((x_p / torch.quantile(img, 0.5)) / torch.log(torch.tensor(2.0, device=img.device)))\n",
    "\n",
    "    output_list = []\n",
    "    exp_value = [c_start, (c_end + c_start) / 2.0, c_end]\n",
    "\n",
    "    for i in range(len(exp_value)):\n",
    "        sc = torch.pow(torch.sqrt(torch.tensor(2.0, device=img.device)), exp_value[i])\n",
    "        img_exp = img * sc\n",
    "        img_pow = img_exp  # Bez zmian, zgodnie z oryginalnym kodem\n",
    "        img_out = torch.where(img_pow > 1.0, torch.ones_like(img_pow), img_pow)\n",
    "        output_list.append(img_out)\n",
    "\n",
    "    return output_list\n",
    "\n",
    "# Przykład użycia:\n",
    "img = torch.randn(1, 3, 32, 32)  # Przykładowy tensor obrazu\n",
    "output_list = mul_exp(img)\n",
    "print(len(output_list))  # 3\n",
    "print(output_list[0].shape)  # [1, 3, 32, 32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 181 HDR images\n",
      "Batch keys: dict_keys(['scale', 'y_ulaw', 'y_rgb', 'filename'])\n",
      "Scale shape: torch.Size([2])\n",
      "y_ulaw shape: torch.Size([2, 3, 256, 256])\n",
      "y_rgb shape: torch.Size([2, 3, 256, 256])\n",
      "Filenames: ['174.exr', '033.exr']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "os.environ['OPENCV_IO_ENABLE_OPENEXR'] = \"1\"\n",
    "\n",
    "class IOException(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "\n",
    "def writeLDR(img, file):\n",
    "    try:\n",
    "        img = cv2.cvtColor(img.astype(np.float32), cv2.COLOR_BGR2RGB)\n",
    "        cv2.imwrite(file, img * 255.0)\n",
    "    except Exception as e:\n",
    "        raise IOException(\"Failed writing LDR image: %s\" % e)\n",
    "\n",
    "\n",
    "def norm(x):\n",
    "    x_max = np.max(x)\n",
    "    x_min = np.min(x)\n",
    "    scale = x_max - x_min\n",
    "    x_norm = (x - x_min) / scale\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "def norm_mean(img):\n",
    "    img = 0.5 * img / img.mean()\n",
    "    return img\n",
    "\n",
    "\n",
    "def ulaw_np(img, scale=10.0):\n",
    "    median_value = np.median(img)\n",
    "    scale = 8.759 * np.power(median_value, 2.148) + 0.1494 * np.power(\n",
    "        median_value, -2.067\n",
    "    )\n",
    "    out = np.log(1 + scale * img) / np.log(1 + scale)\n",
    "    return out, scale\n",
    "\n",
    "\n",
    "def load_hdr_ldr_norm_ulaw(name_hdr):\n",
    "    y = cv2.imread(name_hdr, flags=cv2.IMREAD_ANYDEPTH | cv2.IMREAD_ANYCOLOR)\n",
    "    y_rgb = np.maximum(cv2.cvtColor(y, cv2.COLOR_BGR2RGB), 0.0)\n",
    "    y_rgb = norm_mean(y_rgb)\n",
    "    y_ulaw, scale = ulaw_np(y_rgb)\n",
    "    return scale, y_ulaw, y_rgb\n",
    "\n",
    "\n",
    "class HDRDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            directory (str): Ścieżka do katalogu z plikami HDR.\n",
    "            transform (callable, optional): Opcjonalna transformacja do zastosowania na danych.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "\n",
    "        # Wczytanie listy plików HDR z katalogu\n",
    "        self.hdr_files = [f for f in os.listdir(directory) if f.endswith(\".exr\")]\n",
    "        if not self.hdr_files:\n",
    "            raise ValueError(f\"No HDR files found in directory: {directory}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Zwraca liczbę próbek w datasetcie.\"\"\"\n",
    "        return len(self.hdr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Zwraca próbkę o podanym indeksie.\"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Ścieżka do pliku HDR\n",
    "        hdr_path = os.path.join(self.directory, self.hdr_files[idx])\n",
    "\n",
    "        # Wczytanie i przetworzenie obrazu\n",
    "        scale, y_ulaw, y_rgb = load_hdr_ldr_norm_ulaw(hdr_path)\n",
    "\n",
    "        # Konwersja na tensory PyTorch\n",
    "        y_ulaw = torch.from_numpy(y_ulaw).float()  # [H, W, C]\n",
    "        y_rgb = torch.from_numpy(y_rgb).float()  # [H, W, C]\n",
    "        scale = torch.tensor(scale).float()  # Skalar\n",
    "\n",
    "        # Zamiana osi na format PyTorch [C, H, W]\n",
    "        y_ulaw = y_ulaw.permute(2, 0, 1)\n",
    "        y_rgb = y_rgb.permute(2, 0, 1)\n",
    "\n",
    "        # Zastosowanie opcjonalnej transformacji\n",
    "        if self.transform:\n",
    "            y_ulaw = self.transform(y_ulaw)\n",
    "            y_rgb = self.transform(y_rgb)\n",
    "\n",
    "        # Zwracanie próbki jako słownik\n",
    "        sample = {\n",
    "            \"scale\": scale,\n",
    "            \"y_ulaw\": y_ulaw,\n",
    "            \"y_rgb\": y_rgb,\n",
    "            \"filename\": self.hdr_files[idx],\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Ścieżka do katalogu z plikami HDR\n",
    "data_dir = \"data_resized\"  # Zastąp swoją ścieżką\n",
    "\n",
    "# Inicjalizacja datasetu\n",
    "dataset = HDRDataset(directory=data_dir)\n",
    "print(f\"Loaded {len(dataset)} HDR images\")\n",
    "# Tworzenie DataLoader'a\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iteracja po danych\n",
    "for batch in dataloader:\n",
    "    print(\"Batch keys:\", batch.keys())\n",
    "    print(\"Scale shape:\", batch[\"scale\"].shape)\n",
    "    print(\"y_ulaw shape:\", batch[\"y_ulaw\"].shape)\n",
    "    print(\"y_rgb shape:\", batch[\"y_rgb\"].shape)\n",
    "    print(\"Filenames:\", batch[\"filename\"])\n",
    "    break  # Przerwij po pierwszym batchu dla przykładu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 10, 'batch_size': 2, 'learning_rate': 0.0002, 'decay_rate': 0.9, 'data_dir': 'data_resized', 'validation_split': 0.2, 'valid_step': 1, 'device': 'cuda', 'output_dir': 'output_images'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981dbfe862004fc0a371fdc6f7b7e27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Validation Data:   0%|          | 0/19 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5cb34a10df4c3197e5908aceed438a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/91 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Val Loss: 0.7213, Avg BRISQUE: 22.9678, Avg BRISQUE Reinhard: 21.4222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7239abddc9f48b7b005837bddf5b440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/91 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    235\u001b[39m         scheduler.step()\n\u001b[32m    236\u001b[39m         progress_bar.close()\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    140\u001b[39m     loss.backward()\n\u001b[32m    141\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     progress_bar.set_postfix(stage=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, loss=\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    144\u001b[39m     progress_bar.update(\u001b[32m1\u001b[39m)\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import brisque\n",
    "import numpy as np\n",
    "import torch\n",
    "from brisque import BRISQUE\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Funkcja do obliczania luminancji (przyjmuję, że masz ją zdefiniowaną gdzie indziej, jeśli nie, dodam przykład poniżej)\n",
    "def lum(img):\n",
    "    # Przykład: luminancja jako ważona suma kanałów RGB\n",
    "    return 0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2]\n",
    "\n",
    "\n",
    "def evaluate_image(image) -> float:\n",
    "    metric = BRISQUE(url=False)\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.cpu().detach().numpy()\n",
    "        image = np.transpose(image, (1, 2, 0))  # From (C, H, W) to (H, W, C)\n",
    "        image = (image * 255).clip(0, 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    return metric.score(img=image)\n",
    "\n",
    "\n",
    "def reinhard_tone_mapping(hdr_image):\n",
    "    \"\"\"\n",
    "    Applies Reinhard global tone mapping to an HDR image.\n",
    "    hdr_image: numpy array of shape (H, W, C)\n",
    "    \"\"\"\n",
    "    # Compute luminance\n",
    "    luminance = lum(hdr_image)\n",
    "    # Compute log-average luminance\n",
    "    log_avg_lum = np.exp(np.mean(np.log(luminance + 1e-8)))  # Avoid log(0)\n",
    "    # Scale luminance\n",
    "    scaled_lum = (luminance * (1.0 / log_avg_lum)) / (\n",
    "        1.0 + luminance * (1.0 / log_avg_lum)\n",
    "    )\n",
    "    # Apply to each channel\n",
    "    tone_mapped = np.zeros_like(hdr_image)\n",
    "    for c in range(3):\n",
    "        tone_mapped[:, :, c] = (hdr_image[:, :, c] / (luminance + 1e-8)) * scaled_lum\n",
    "    return np.clip(tone_mapped, 0, 1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"decay_rate\": 0.9,\n",
    "    \"data_dir\": \"data_resized\",\n",
    "    \"validation_split\": 0.2,\n",
    "    \"valid_step\": 1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"output_dir\": \"output_images\",\n",
    "}\n",
    "\n",
    "\n",
    "def train(config: dict) -> None:\n",
    "    print(config)\n",
    "    device = torch.device(config[\"device\"])\n",
    "\n",
    "    # Dataset i dataloaders\n",
    "    base_dataset = HDRDataset(directory=config[\"data_dir\"])\n",
    "    train_size = int((1 - config[\"validation_split\"]) * len(base_dataset))\n",
    "    val_size = len(base_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(base_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    # Model, optimizer, scheduler\n",
    "    model = TMONet(input_channels=3).to(device)\n",
    "    feature_extractor = VGG19FeatureExtractor(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    scheduler = ExponentialLR(optimizer, gamma=config[\"decay_rate\"])\n",
    "\n",
    "    os.makedirs(config[\"output_dir\"], exist_ok=True)\n",
    "    reference_dir = os.path.join(config[\"output_dir\"], \"reference\")\n",
    "    os.makedirs(reference_dir, exist_ok=True)\n",
    "\n",
    "    total_batches = len(train_loader) + (\n",
    "        len(val_loader) if config[\"valid_step\"] > 0 else 0\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        brisque_total = 0.0\n",
    "        num_samples = 0\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(val_loader, desc=\"Processing Validation Data\", unit=\"batch\")\n",
    "        ):\n",
    "            hdr_rgb = batch[\"y_rgb\"].float().to(device)  # Ground truth HDR images\n",
    "\n",
    "            for i in range(hdr_rgb.size(0)):\n",
    "                # Convert tensor to numpy\n",
    "                hdr_np = hdr_rgb[i].cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n",
    "\n",
    "                # Apply Reinhard tone mapping\n",
    "                tone_mapped_np = reinhard_tone_mapping(hdr_np)\n",
    "\n",
    "                # Convert back to tensor for saving\n",
    "                tone_mapped_tensor = torch.from_numpy(\n",
    "                    tone_mapped_np.transpose(2, 0, 1)\n",
    "                ).float()  # (C, H, W)\n",
    "\n",
    "                # Save the image\n",
    "                img_filename = os.path.join(\n",
    "                    reference_dir, f\"val_{batch_idx * config['batch_size'] + i}.png\"\n",
    "                )\n",
    "                save_image(tone_mapped_tensor, img_filename)\n",
    "\n",
    "                # Calculate BRISQUE score\n",
    "                brisque_score = evaluate_image(tone_mapped_tensor)\n",
    "                brisque_total += brisque_score\n",
    "                num_samples += 1\n",
    "\n",
    "    # Compute average BRISQUE score\n",
    "    reinhard_brisque_score = brisque_total / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        progress_bar = tqdm(\n",
    "            total=total_batches, desc=f\"Epoch {epoch}/{config['epochs']}\", unit=\"batch\"\n",
    "        )\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            hdr_ulaw = batch[\"y_ulaw\"].float().to(device)\n",
    "            hdr_rgb = batch[\"y_rgb\"].float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_list = mul_exp(hdr_ulaw)\n",
    "            output = model(output_list[0], output_list[1], output_list[2])\n",
    "\n",
    "            loss = FCM_loss(output, hdr_rgb, feature_extractor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix(stage=\"Training\", loss=loss.item())\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        # Validation phase\n",
    "        if epoch % config[\"valid_step\"] == 0:\n",
    "            os.makedirs(os.path.join(config[\"output_dir\"], f\"{epoch}\"), exist_ok=True)\n",
    "            model.eval()\n",
    "            val_loss_total = 0.0\n",
    "            val_brisque_total = 0.0\n",
    "            num_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(val_loader):\n",
    "                    hdr_ulaw = batch[\"y_ulaw\"].float().to(device)\n",
    "                    hdr_rgb = batch[\"y_rgb\"].float().to(device)\n",
    "\n",
    "                    output_list = mul_exp(hdr_ulaw)\n",
    "                    output = model(output_list[0], output_list[1], output_list[2])\n",
    "\n",
    "                    val_loss = FCM_loss(output, hdr_rgb, feature_extractor)\n",
    "                    val_loss_total += val_loss.item() * output.size(0)\n",
    "\n",
    "                    # Przetwarzanie obrazów w batchu\n",
    "                    for i in range(output.size(0)):\n",
    "                        # Konwersja tensorów na numpy\n",
    "                        xx = hdr_ulaw[i].cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n",
    "                        yy = hdr_rgb[i].cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n",
    "                        y_pred = output[i].cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n",
    "\n",
    "                        # Parametr a z pierwszego kodu\n",
    "                        a = 0.6\n",
    "\n",
    "                        # Rozdziel kanały RGB\n",
    "                        r = yy[:, :, 0]\n",
    "                        g = yy[:, :, 1]\n",
    "                        b = yy[:, :, 2]\n",
    "\n",
    "                        # Oblicz luminancję\n",
    "                        y_gt_lum_np = lum(yy)\n",
    "                        yy_predict_np_lum = lum(y_pred)\n",
    "\n",
    "                        # Transformacja obrazu\n",
    "                        img_out = np.zeros(np.shape(yy))\n",
    "                        img_out[:, :, 0] = (\n",
    "                            r / (y_gt_lum_np + 1e-8)\n",
    "                        ) ** a * yy_predict_np_lum\n",
    "                        img_out[:, :, 1] = (\n",
    "                            g / (y_gt_lum_np + 1e-8)\n",
    "                        ) ** a * yy_predict_np_lum\n",
    "                        img_out[:, :, 2] = (\n",
    "                            b / (y_gt_lum_np + 1e-8)\n",
    "                        ) ** a * yy_predict_np_lum\n",
    "\n",
    "                        # Dodaj małą wartość w mianowniku, aby uniknąć dzielenia przez 0\n",
    "                        img_out = np.clip(\n",
    "                            img_out, 0, 1\n",
    "                        )  # Ograniczenie do zakresu [0, 1]\n",
    "\n",
    "                        # Konwersja z powrotem na tensor do zapisu\n",
    "                        img_tensor = torch.from_numpy(\n",
    "                            img_out.transpose(2, 0, 1)\n",
    "                        ).float()  # (C, H, W)\n",
    "\n",
    "                        # Ocena BRISQUE\n",
    "                        brisque_score = evaluate_image(img_tensor)\n",
    "                        val_brisque_total += brisque_score\n",
    "\n",
    "                        # Zapis obrazu\n",
    "                        img_filename = os.path.join(\n",
    "                            config[\"output_dir\"],\n",
    "                            f\"{epoch}/{batch_idx * config['batch_size'] + i}.png\",\n",
    "                        )\n",
    "                        save_image(img_tensor, img_filename)\n",
    "\n",
    "                    num_samples += output.size(0)\n",
    "                    progress_bar.set_postfix(stage=\"Validation\")\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "            # Średnie metryki\n",
    "            avg_val_loss = val_loss_total / num_samples\n",
    "            avg_brisque_score = val_brisque_total / num_samples\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                stage=\"Validation\",\n",
    "                val_loss=avg_val_loss,\n",
    "                val_brisque=avg_brisque_score,\n",
    "                reinhard_brisque_score=reinhard_brisque_score,\n",
    "            )\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{config['epochs']} - Avg Val Loss: {avg_val_loss:.4f}, Avg BRISQUE: {avg_brisque_score:.4f}, Avg BRISQUE Reinhard: {reinhard_brisque_score:.4f}\"\n",
    "            )\n",
    "\n",
    "        scheduler.step()\n",
    "        progress_bar.close()\n",
    "\n",
    "\n",
    "train(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
