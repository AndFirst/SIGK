{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3440d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.camera_pos = [5.0, 5.0, 15.0]\n",
    "        \n",
    "        self.max_r = math.sqrt(25**2 + 25**2 + 35**2)  # Max radial distance\n",
    "        self.r_range = [0, self.max_r]\n",
    "        self.theta_range = [0, math.pi]\n",
    "        self.phi_range = [-math.pi, math.pi]\n",
    "        self.shininess_range = [3, 20]\n",
    "        self.diffuse_range = [0, 1]\n",
    "        \n",
    "        self.image_files = [f\"image_{str(i).zfill(4)}.png\" for i in self.data['frame']]\n",
    "        \n",
    "        self.params_tensor = self.preprocess_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def normalize(self, value, range_min, range_max):\n",
    "        \"\"\"Normalize value to [-1, 1].\"\"\"\n",
    "        normalized = (value - range_min) / (range_max - range_min)\n",
    "        return 2 * normalized - 1\n",
    "    \n",
    "    def get_orthonormal_basis(self, z_prime):\n",
    "        \"\"\"Compute orthonormal basis with z_prime as the z'-axis.\"\"\"\n",
    "        ref = [0, 0, 1]\n",
    "        if abs(z_prime[2]) > 0.99:\n",
    "            ref = [0, 1, 0]\n",
    "        \n",
    "        x_prime = [\n",
    "            z_prime[1] * ref[2] - z_prime[2] * ref[1],\n",
    "            z_prime[2] * ref[0] - z_prime[0] * ref[2],\n",
    "            z_prime[0] * ref[1] - z_prime[1] * ref[0]\n",
    "        ]\n",
    "        x_norm = math.sqrt(sum(x * x for x in x_prime))\n",
    "        x_prime = [x / x_norm for x in x_prime]\n",
    "        \n",
    "        y_prime = [\n",
    "            z_prime[1] * x_prime[2] - z_prime[2] * x_prime[1],\n",
    "            z_prime[2] * x_prime[0] - z_prime[0] * x_prime[2],\n",
    "            z_prime[0] * x_prime[1] - z_prime[1] * x_prime[0]\n",
    "        ]\n",
    "        y_norm = math.sqrt(sum(y * y for y in y_prime))\n",
    "        y_prime = [y / y_norm for y in y_prime]\n",
    "        \n",
    "        return x_prime, y_prime, z_prime\n",
    "    \n",
    "    def cartesian_to_spherical_lookat(self, x, y, z, lookat_vec):\n",
    "        \"\"\"Convert relative Cartesian coordinates to spherical coordinates aligned with lookat vector.\"\"\"\n",
    "        x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "        \n",
    "        x_new = x * x_prime[0] + y * x_prime[1] + z * x_prime[2]\n",
    "        y_new = x * y_prime[0] + y * y_prime[1] + z * y_prime[2]\n",
    "        z_new = x * z_prime[0] + y * z_prime[1] + z * z_prime[2]\n",
    "        \n",
    "        # Convert to spherical coordinates\n",
    "        r = math.sqrt(x_new**2 + y_new**2 + z_new**2)\n",
    "        theta = math.acos(z_new / r) if r > 0 else 0\n",
    "        phi = math.atan2(y_new, x_new)\n",
    "        \n",
    "        return r, theta, phi\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess all data to compute spherical coordinates and additional relative features.\"\"\"\n",
    "        params_list = []\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            row = self.data.iloc[idx]\n",
    "\n",
    "            lookat_x_rel = row['lookat_target_x'] - self.camera_pos[0]\n",
    "            lookat_y_rel = row['lookat_target_y'] - self.camera_pos[1]\n",
    "            lookat_z_rel = row['lookat_target_z'] - self.camera_pos[2]\n",
    "            lookat_norm = math.sqrt(lookat_x_rel**2 + lookat_y_rel**2 + lookat_z_rel**2)\n",
    "            lookat_vec = (\n",
    "                lookat_x_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_y_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_z_rel / lookat_norm if lookat_norm > 0 else 0\n",
    "            )\n",
    "\n",
    "            model_x_rel = row['model_translation_x'] - self.camera_pos[0]\n",
    "            model_y_rel = row['model_translation_y'] - self.camera_pos[1]\n",
    "            model_z_rel = row['model_translation_z'] - self.camera_pos[2]\n",
    "            model_r, model_theta, model_phi = self.cartesian_to_spherical_lookat(\n",
    "                model_x_rel, model_y_rel, model_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_x_rel = row['light_position_x'] - self.camera_pos[0]\n",
    "            light_y_rel = row['light_position_y'] - self.camera_pos[1]\n",
    "            light_z_rel = row['light_position_z'] - self.camera_pos[2]\n",
    "            light_r, light_theta, light_phi = self.cartesian_to_spherical_lookat(\n",
    "                light_x_rel, light_y_rel, light_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_to_model_vec = [\n",
    "                light_x_rel - model_x_rel,\n",
    "                light_y_rel - model_y_rel,\n",
    "                light_z_rel - model_z_rel\n",
    "            ]\n",
    "            ltm_norm = math.sqrt(sum(x**2 for x in light_to_model_vec))\n",
    "            light_to_model_unit = [x / ltm_norm if ltm_norm > 0 else 0 for x in light_to_model_vec]\n",
    "\n",
    "            cos_view_light = sum(l * v for l, v in zip(light_to_model_unit, lookat_vec))\n",
    "            ltm_normalized = self.normalize(ltm_norm, 0, self.max_r)\n",
    "\n",
    "            x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "            def rotate_to_view(x, y, z):\n",
    "                return [\n",
    "                    x * x_prime[0] + y * x_prime[1] + z * x_prime[2],\n",
    "                    x * y_prime[0] + y * y_prime[1] + z * y_prime[2],\n",
    "                    x * z_prime[0] + y * z_prime[1] + z * z_prime[2],\n",
    "                ]\n",
    "            model_view_xyz = rotate_to_view(model_x_rel, model_y_rel, model_z_rel)\n",
    "            light_view_xyz = rotate_to_view(light_x_rel, light_y_rel, light_z_rel)\n",
    "\n",
    "            params = [\n",
    "                self.normalize(model_r, *self.r_range),\n",
    "                self.normalize(model_theta, *self.theta_range),\n",
    "                self.normalize(model_phi, *self.phi_range),\n",
    "                self.normalize(light_r, *self.r_range),\n",
    "                self.normalize(light_theta, *self.theta_range),\n",
    "                self.normalize(light_phi, *self.phi_range),\n",
    "                lookat_vec[0],\n",
    "                lookat_vec[1],\n",
    "                lookat_vec[2],\n",
    "                self.normalize(row['material_diffuse_r'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_g'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_b'], *self.diffuse_range),\n",
    "                self.normalize(row['material_shininess'], *self.shininess_range),\n",
    "                ltm_normalized,\n",
    "                cos_view_light,\n",
    "                *light_to_model_unit,\n",
    "                *model_view_xyz,\n",
    "                *light_view_xyz\n",
    "            ]\n",
    "\n",
    "            params_list.append(params)\n",
    "\n",
    "        return torch.tensor(params_list, dtype=torch.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        params_tensor = self.params_tensor[idx]\n",
    "        \n",
    "        return image, params_tensor\n",
    "    \n",
    "def denormalize(img):\n",
    "    \"\"\"Denormalize images from [-1, 1] to [0, 1] for visualization.\"\"\"\n",
    "    return (img * 0.5) + 0.5\n",
    "\n",
    "def get_data_loaders(csv_path, image_dir, batch_size=32, train_split=0.8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    dataset = FrameDataset(csv_path, image_dir, transform=transform)\n",
    "    \n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "csv_path = \"../output/frame_parameters.csv\"\n",
    "image_dir = \"../output\"\n",
    "batch_size = 64\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(csv_path, image_dir, batch_size)\n",
    "\n",
    "for images, params in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Parameters batch shape: {params.shape}\")\n",
    "    \n",
    "    min_val = torch.min(params)\n",
    "    max_val = torch.max(params)\n",
    "    std_val = torch.std(params)\n",
    "    median_val = torch.median(params)\n",
    "    \n",
    "    print(f\"Min: {min_val.item():.4f}, Max: {max_val.item():.4f}, \"\n",
    "          f\"Std: {std_val.item():.4f}, Median: {median_val.item():.4f}\")\n",
    "    \n",
    "    num_samples = min(4, images.size(0))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        img = denormalize(images[i])\n",
    "        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "            img = img.permute(1, 2, 0)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img.squeeze().numpy(), cmap='gray' if img.shape[-1] == 1 else None)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualize_progress(diffusion_model, val_loader, epoch, device, num_samples=5):\n",
    "    diffusion_model.model.eval()\n",
    "\n",
    "    images, params = next(iter(val_loader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    params = params[:num_samples].to(device)\n",
    "    \n",
    "    generated_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_samples), desc=\"Generating images\"):\n",
    "            gen_img = diffusion_model.sample(params[i:i+1], img_shape=(3, 128, 128))\n",
    "            generated_images.append(gen_img.squeeze(0))\n",
    "    \n",
    "    generated_images = torch.stack(generated_images)\n",
    "    \n",
    "    generated_images = denormalize(generated_images).cpu()\n",
    "    ground_truth_images = denormalize(images).cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4 * num_samples))\n",
    "    fig.suptitle(f'Epoch {epoch}: Generated vs Ground Truth', fontsize=16)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        gen_img = generated_images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i, 0].imshow(gen_img)\n",
    "        axes[i, 0].set_title('Generated')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        gt_img = ground_truth_images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i, 1].imshow(gt_img)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = channels // num_heads\n",
    "        self.query = nn.Conv2d(channels, channels, 1)\n",
    "        self.key = nn.Conv2d(channels, channels, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.out = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, c, h, w = x.size()\n",
    "        q = self.query(x).view(batch, self.num_heads, self.head_dim, h * w).permute(0, 1, 3, 2)\n",
    "        k = self.key(x).view(batch, self.num_heads, self.head_dim, h * w)\n",
    "        v = self.value(x).view(batch, self.num_heads, self.head_dim, h * w)\n",
    "        attn = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5), dim=-1)\n",
    "        out = torch.matmul(attn, v).permute(0, 1, 3, 2).contiguous().view(batch, c, h, w)\n",
    "        out = self.out(out)\n",
    "        return x + self.gamma * out\n",
    "    \n",
    "class TransformerCondEmb(nn.Module):\n",
    "    def __init__(self, cond_dim, base_channels, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(cond_dim, base_channels)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=base_channels, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.out = nn.Linear(base_channels, base_channels * 4)\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        x = self.embedding(cond).unsqueeze(1)  # Add sequence dim\n",
    "        x = self.transformer(x).squeeze(1)\n",
    "        return self.out(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, cond_dim=24, base_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_dim = cond_dim\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, base_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(base_channels, base_channels * 4)\n",
    "        )\n",
    "        \n",
    "        self.cond_emb = TransformerCondEmb(cond_dim, base_channels)\n",
    "        \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 4, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 4, 3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 4),\n",
    "            MultiHeadSelfAttention(base_channels * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 4, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels * 4, base_channels, 4, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Conv2d(base_channels * 2, in_channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, t, cond):\n",
    "        t = t.unsqueeze(-1)\n",
    "        t_emb = self.time_emb(t)\n",
    "        \n",
    "        cond_emb = self.cond_emb(cond)\n",
    "        \n",
    "        emb = t_emb + cond_emb\n",
    "        \n",
    "        h1 = self.enc1(x)\n",
    "        h2 = self.enc2(h1)\n",
    "        h3 = self.enc3(h2)\n",
    "        \n",
    "        h = self.mid(h3)\n",
    "        h = h + emb.view(-1, h.shape[1], 1, 1)\n",
    "        \n",
    "        h = self.dec1(h)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = self.dec2(h)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        h = self.dec3(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, model, timesteps=512, beta_start=0.0001, beta_end=0.02, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def add_noise(self, x, t):\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t])[:, None, None, None]\n",
    "        noise = torch.randn_like(x)\n",
    "        return sqrt_alpha_bar * x + sqrt_one_minus_alpha_bar * noise, noise\n",
    "    \n",
    "    def train_step(self, x, cond, optimizer):\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        x_noisy, noise = self.add_noise(x, t)\n",
    "        predicted_noise = self.model(x_noisy, t / self.timesteps, cond)\n",
    "        \n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, img_shape=(3, 128, 128)):\n",
    "        x = torch.randn((1, *img_shape), device=self.device)\n",
    "        cond = cond.to(device)\n",
    "        \n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.full((1,), t / self.timesteps, device=self.device, dtype=torch.float32)\n",
    "            predicted_noise = self.model(x, t_tensor, cond)\n",
    "            \n",
    "            alpha = self.alphas[t]\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            beta = self.betas[t]\n",
    "            \n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "            \n",
    "            x = (1 / torch.sqrt(alpha)) * (\n",
    "                x - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise\n",
    "            ) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        print(x.min(), x.max())\n",
    "        return torch.tanh(x)\n",
    "\n",
    "def train_diffusion(train_loader, val_loader, epochs=100, device='cuda', vis_interval=10,\n",
    "                    checkpoint_dir='checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    latest_ckpt = os.path.join(checkpoint_dir, 'latest.pt')\n",
    "    best_ckpt = os.path.join(checkpoint_dir, 'best.pt')\n",
    "\n",
    "    model = UNet(in_channels=3, cond_dim=24)\n",
    "    diffusion = DiffusionModel(model, timesteps=1000, device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if os.path.exists(latest_ckpt):\n",
    "        checkpoint = torch.load(latest_ckpt, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', best_val_loss)\n",
    "        print(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, params in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            params = params.to(device)\n",
    "            loss = diffusion.train_step(images, params, optimizer)\n",
    "            train_loss += loss\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, params in val_loader:\n",
    "                images = images.to(device)\n",
    "                params = params.to(device)\n",
    "                batch_size = images.shape[0]\n",
    "                t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device)\n",
    "                x_noisy, noise = diffusion.add_noise(images, t)\n",
    "                predicted_noise = model(x_noisy, t / diffusion.timesteps, params)\n",
    "                val_loss += F.mse_loss(predicted_noise, noise).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_val_loss': best_val_loss}, best_ckpt)\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_val_loss': best_val_loss}, latest_ckpt)\n",
    "\n",
    "        if (epoch + 1) % vis_interval == 0:\n",
    "            print(f\"Generating visualization for epoch {epoch+1}...\")\n",
    "            visualize_progress(diffusion, val_loader, epoch + 1, device, num_samples=5)\n",
    "\n",
    "    return diffusion\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "diffusion = train_diffusion(train_loader, val_loader, epochs=200, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
