{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3440d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([64, 3, 128, 128])\n",
      "Parameters batch shape: torch.Size([64, 24])\n",
      "Min: -26.7018, Max: 41.5652, Std: 7.3682, Median: -0.0354\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.camera_pos = [5.0, 5.0, 15.0]\n",
    "        \n",
    "        self.max_r = math.sqrt(25**2 + 25**2 + 35**2)  # Max radial distance\n",
    "        self.r_range = [0, self.max_r]\n",
    "        self.theta_range = [0, math.pi]\n",
    "        self.phi_range = [-math.pi, math.pi]\n",
    "        self.shininess_range = [3, 20]\n",
    "        self.diffuse_range = [0, 1]\n",
    "        \n",
    "        self.image_files = [f\"image_{str(i).zfill(4)}.png\" for i in self.data['frame']]\n",
    "        \n",
    "        self.params_tensor = self.preprocess_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def normalize(self, value, range_min, range_max):\n",
    "        \"\"\"Normalize value to [-1, 1].\"\"\"\n",
    "        normalized = (value - range_min) / (range_max - range_min)\n",
    "        return 2 * normalized - 1\n",
    "    \n",
    "    def get_orthonormal_basis(self, z_prime):\n",
    "        \"\"\"Compute orthonormal basis with z_prime as the z'-axis.\"\"\"\n",
    "        ref = [0, 0, 1]\n",
    "        if abs(z_prime[2]) > 0.99:\n",
    "            ref = [0, 1, 0]\n",
    "        \n",
    "        x_prime = [\n",
    "            z_prime[1] * ref[2] - z_prime[2] * ref[1],\n",
    "            z_prime[2] * ref[0] - z_prime[0] * ref[2],\n",
    "            z_prime[0] * ref[1] - z_prime[1] * ref[0]\n",
    "        ]\n",
    "        x_norm = math.sqrt(sum(x * x for x in x_prime))\n",
    "        x_prime = [x / x_norm for x in x_prime]\n",
    "        \n",
    "        y_prime = [\n",
    "            z_prime[1] * x_prime[2] - z_prime[2] * x_prime[1],\n",
    "            z_prime[2] * x_prime[0] - z_prime[0] * x_prime[2],\n",
    "            z_prime[0] * x_prime[1] - z_prime[1] * x_prime[0]\n",
    "        ]\n",
    "        y_norm = math.sqrt(sum(y * y for y in y_prime))\n",
    "        y_prime = [y / y_norm for y in y_prime]\n",
    "        \n",
    "        return x_prime, y_prime, z_prime\n",
    "    \n",
    "    def cartesian_to_spherical_lookat(self, x, y, z, lookat_vec):\n",
    "        \"\"\"Convert relative Cartesian coordinates to spherical coordinates aligned with lookat vector.\"\"\"\n",
    "        x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "        \n",
    "        x_new = x * x_prime[0] + y * x_prime[1] + z * x_prime[2]\n",
    "        y_new = x * y_prime[0] + y * y_prime[1] + z * y_prime[2]\n",
    "        z_new = x * z_prime[0] + y * z_prime[1] + z * z_prime[2]\n",
    "        \n",
    "        # Convert to spherical coordinates\n",
    "        r = math.sqrt(x_new**2 + y_new**2 + z_new**2)\n",
    "        theta = math.acos(z_new / r) if r > 0 else 0\n",
    "        phi = math.atan2(y_new, x_new)\n",
    "        \n",
    "        return r, theta, phi\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess all data to compute spherical coordinates and additional relative features.\"\"\"\n",
    "        params_list = []\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            row = self.data.iloc[idx]\n",
    "\n",
    "            lookat_x_rel = row['lookat_target_x'] - self.camera_pos[0]\n",
    "            lookat_y_rel = row['lookat_target_y'] - self.camera_pos[1]\n",
    "            lookat_z_rel = row['lookat_target_z'] - self.camera_pos[2]\n",
    "            lookat_norm = math.sqrt(lookat_x_rel**2 + lookat_y_rel**2 + lookat_z_rel**2)\n",
    "            lookat_vec = (\n",
    "                lookat_x_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_y_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_z_rel / lookat_norm if lookat_norm > 0 else 0\n",
    "            )\n",
    "\n",
    "            model_x_rel = row['model_translation_x'] - self.camera_pos[0]\n",
    "            model_y_rel = row['model_translation_y'] - self.camera_pos[1]\n",
    "            model_z_rel = row['model_translation_z'] - self.camera_pos[2]\n",
    "            model_r, model_theta, model_phi = self.cartesian_to_spherical_lookat(\n",
    "                model_x_rel, model_y_rel, model_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_x_rel = row['light_position_x'] - self.camera_pos[0]\n",
    "            light_y_rel = row['light_position_y'] - self.camera_pos[1]\n",
    "            light_z_rel = row['light_position_z'] - self.camera_pos[2]\n",
    "            light_r, light_theta, light_phi = self.cartesian_to_spherical_lookat(\n",
    "                light_x_rel, light_y_rel, light_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_to_model_vec = [\n",
    "                light_x_rel - model_x_rel,\n",
    "                light_y_rel - model_y_rel,\n",
    "                light_z_rel - model_z_rel\n",
    "            ]\n",
    "            ltm_norm = math.sqrt(sum(x**2 for x in light_to_model_vec))\n",
    "            light_to_model_unit = [x / ltm_norm if ltm_norm > 0 else 0 for x in light_to_model_vec]\n",
    "\n",
    "            cos_view_light = sum(l * v for l, v in zip(light_to_model_unit, lookat_vec))\n",
    "            ltm_normalized = self.normalize(ltm_norm, 0, self.max_r)\n",
    "\n",
    "            x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "            def rotate_to_view(x, y, z):\n",
    "                return [\n",
    "                    x * x_prime[0] + y * x_prime[1] + z * x_prime[2],\n",
    "                    x * y_prime[0] + y * y_prime[1] + z * y_prime[2],\n",
    "                    x * z_prime[0] + y * z_prime[1] + z * z_prime[2],\n",
    "                ]\n",
    "            model_view_xyz = rotate_to_view(model_x_rel, model_y_rel, model_z_rel)\n",
    "            light_view_xyz = rotate_to_view(light_x_rel, light_y_rel, light_z_rel)\n",
    "\n",
    "            params = [\n",
    "                self.normalize(model_r, *self.r_range),\n",
    "                self.normalize(model_theta, *self.theta_range),\n",
    "                self.normalize(model_phi, *self.phi_range),\n",
    "                self.normalize(light_r, *self.r_range),\n",
    "                self.normalize(light_theta, *self.theta_range),\n",
    "                self.normalize(light_phi, *self.phi_range),\n",
    "                lookat_vec[0],\n",
    "                lookat_vec[1],\n",
    "                lookat_vec[2],\n",
    "                self.normalize(row['material_diffuse_r'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_g'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_b'], *self.diffuse_range),\n",
    "                self.normalize(row['material_shininess'], *self.shininess_range),\n",
    "                ltm_normalized,\n",
    "                cos_view_light,\n",
    "                *light_to_model_unit,\n",
    "                *model_view_xyz,\n",
    "                *light_view_xyz\n",
    "            ]\n",
    "\n",
    "            params_list.append(params)\n",
    "\n",
    "        return torch.tensor(params_list, dtype=torch.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        params_tensor = self.params_tensor[idx]\n",
    "        \n",
    "        return image, params_tensor\n",
    "    \n",
    "def denormalize(img):\n",
    "    \"\"\"Denormalize images from [-1, 1] to [0, 1] for visualization.\"\"\"\n",
    "    return (img * 0.5) + 0.5\n",
    "\n",
    "def get_data_loaders(csv_path, image_dir, batch_size=32, train_split=0.8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    dataset = FrameDataset(csv_path, image_dir, transform=transform)\n",
    "    \n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "csv_path = \"../output/frame_parameters.csv\"\n",
    "image_dir = \"../output\"\n",
    "batch_size = 64\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(csv_path, image_dir, batch_size)\n",
    "\n",
    "for images, params in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Parameters batch shape: {params.shape}\")\n",
    "    \n",
    "    min_val = torch.min(params)\n",
    "    max_val = torch.max(params)\n",
    "    std_val = torch.std(params)\n",
    "    median_val = torch.median(params)\n",
    "    \n",
    "    print(f\"Min: {min_val.item():.4f}, Max: {max_val.item():.4f}, \"\n",
    "          f\"Std: {std_val.item():.4f}, Median: {median_val.item():.4f}\")\n",
    "    \n",
    "    num_samples = min(4, images.size(0))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        img = denormalize(images[i])\n",
    "        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "            img = img.permute(1, 2, 0)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img.squeeze().numpy(), cmap='gray' if img.shape[-1] == 1 else None)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a8d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def visualize_progress(diffusion_model, val_loader, epoch, device, num_samples=5):\n",
    "    diffusion_model.model.eval()\n",
    "\n",
    "    images, params = next(iter(val_loader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    params = params[:num_samples].to(device)\n",
    "    \n",
    "    generated_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(num_samples), desc=\"Generating images\"):\n",
    "            gen_img = diffusion_model.sample(params[i:i+1], img_shape=(3, 128, 128))\n",
    "            generated_images.append(gen_img.squeeze(0))\n",
    "    \n",
    "    generated_images = torch.stack(generated_images)\n",
    "    \n",
    "    generated_images = denormalize(generated_images).cpu()\n",
    "    ground_truth_images = denormalize(images).cpu()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4 * num_samples))\n",
    "    fig.suptitle(f'Epoch {epoch}: Generated vs Ground Truth', fontsize=16)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        gen_img = generated_images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i, 0].imshow(gen_img)\n",
    "        axes[i, 0].set_title('Generated')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        gt_img = ground_truth_images[i].permute(1, 2, 0).numpy()\n",
    "        axes[i, 1].imshow(gt_img)\n",
    "        axes[i, 1].set_title('Ground Truth')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ea02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idab/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "Epoch 1/200:   0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 233\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m diffusion\n\u001b[32m    232\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m diffusion = \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mtrain_diffusion\u001b[39m\u001b[34m(train_loader, val_loader, epochs, device, vis_interval, checkpoint_dir)\u001b[39m\n\u001b[32m    192\u001b[39m     images = images.to(device)\n\u001b[32m    193\u001b[39m     params = params.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     loss = \u001b[43mdiffusion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     train_loss += loss\n\u001b[32m    196\u001b[39m train_loss /= \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mDiffusionModel.train_step\u001b[39m\u001b[34m(self, x, cond, optimizer)\u001b[39m\n\u001b[32m    127\u001b[39m t = torch.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.timesteps, (batch_size,), device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    129\u001b[39m x_noisy, noise = \u001b[38;5;28mself\u001b[39m.add_noise(x, t)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m predicted_noise = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m loss = F.mse_loss(predicted_noise, noise)\n\u001b[32m    134\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x, t, cond)\u001b[39m\n\u001b[32m     96\u001b[39m h3 = \u001b[38;5;28mself\u001b[39m.enc3(h2)\n\u001b[32m     98\u001b[39m h = \u001b[38;5;28mself\u001b[39m.mid(h3)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m h = \u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m h = \u001b[38;5;28mself\u001b[39m.dec1(h)\n\u001b[32m    102\u001b[39m h = torch.cat([h, h2], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, 1)\n",
    "        self.value = nn.Conv2d(channels, channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, c, h, w = x.size()\n",
    "        q = self.query(x).view(batch, -1, h * w).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch, -1, h * w)\n",
    "        v = self.value(x).view(batch, -1, h * w)\n",
    "        attn = F.softmax(torch.bmm(q, k) / (c // 8) ** 0.5, dim=-1)\n",
    "        out = torch.bmm(v, attn.permute(0, 2, 1)).view(batch, c, h, w)\n",
    "        return x + self.gamma * out\n",
    "    \n",
    "class TransformerCondEmb(nn.Module):\n",
    "    def __init__(self, cond_dim, base_channels, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(cond_dim, base_channels)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=base_channels, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.out = nn.Linear(base_channels, base_channels * 4)\n",
    "    \n",
    "    def forward(self, cond):\n",
    "        x = self.embedding(cond).unsqueeze(1)  # Add sequence dim\n",
    "        x = self.transformer(x).squeeze(1)\n",
    "        return self.out(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, cond_dim=24, base_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.cond_dim = cond_dim\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "        self.time_emb = nn.Sequential(\n",
    "            nn.Linear(1, base_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(base_channels, base_channels * 4)\n",
    "        )\n",
    "        \n",
    "        self.cond_emb = TransformerCondEmb(cond_dim, base_channels)\n",
    "        \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 4, 3, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mid = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 4, 3, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 4),\n",
    "            SelfAttention(base_channels * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 4, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(base_channels * 4, base_channels, 4, stride=2, padding=1),\n",
    "            nn.GroupNorm(num_groups=32, num_channels=base_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Conv2d(base_channels * 2, in_channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, t, cond):\n",
    "        t = t.unsqueeze(-1)\n",
    "        t_emb = self.time_emb(t)\n",
    "        \n",
    "        cond_emb = self.cond_emb(cond)\n",
    "        \n",
    "        emb = t_emb + cond_emb\n",
    "        \n",
    "        h1 = self.enc1(x)\n",
    "        h2 = self.enc2(h1)\n",
    "        h3 = self.enc3(h2)\n",
    "        \n",
    "        h = self.mid(h3)\n",
    "        h = h + emb.view(-1, h.shape[1], 1, 1)\n",
    "        \n",
    "        h = self.dec1(h)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = self.dec2(h)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        h = self.dec3(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class DiffusionModel:\n",
    "    def __init__(self, model, timesteps=512, beta_start=0.0001, beta_end=0.02, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def add_noise(self, x, t):\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t])[:, None, None, None]\n",
    "        noise = torch.randn_like(x)\n",
    "        return sqrt_alpha_bar * x + sqrt_one_minus_alpha_bar * noise, noise\n",
    "    \n",
    "    def train_step(self, x, cond, optimizer):\n",
    "        batch_size = x.shape[0]\n",
    "        t = torch.randint(0, self.timesteps, (batch_size,), device=self.device)\n",
    "        \n",
    "        x_noisy, noise = self.add_noise(x, t)\n",
    "        predicted_noise = self.model(x_noisy, t / self.timesteps, cond)\n",
    "        \n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, img_shape=(3, 128, 128)):\n",
    "        x = torch.randn((1, *img_shape), device=self.device)\n",
    "        cond = cond.to(device)\n",
    "        \n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.full((1,), t / self.timesteps, device=self.device, dtype=torch.float32)\n",
    "            predicted_noise = self.model(x, t_tensor, cond)\n",
    "            \n",
    "            alpha = self.alphas[t]\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            beta = self.betas[t]\n",
    "            \n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x)\n",
    "            \n",
    "            x = (1 / torch.sqrt(alpha)) * (\n",
    "                x - ((1 - alpha) / torch.sqrt(1 - alpha_bar)) * predicted_noise\n",
    "            ) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        print(x.min(), x.max())\n",
    "        return torch.tanh(x)\n",
    "\n",
    "def train_diffusion(train_loader, val_loader, epochs=100, device='cuda', vis_interval=10,\n",
    "                    checkpoint_dir='checkpoints'):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    latest_ckpt = os.path.join(checkpoint_dir, 'latest.pt')\n",
    "    best_ckpt = os.path.join(checkpoint_dir, 'best.pt')\n",
    "\n",
    "    model = UNet(in_channels=3, cond_dim=24)\n",
    "    diffusion = DiffusionModel(model, timesteps=1000, device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if os.path.exists(latest_ckpt):\n",
    "        checkpoint = torch.load(latest_ckpt, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint.get('best_val_loss', best_val_loss)\n",
    "        print(f\"Resumed from epoch {start_epoch}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, params in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            params = params.to(device)\n",
    "            loss = diffusion.train_step(images, params, optimizer)\n",
    "            train_loss += loss\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, params in val_loader:\n",
    "                images = images.to(device)\n",
    "                params = params.to(device)\n",
    "                batch_size = images.shape[0]\n",
    "                t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device)\n",
    "                x_noisy, noise = diffusion.add_noise(images, t)\n",
    "                predicted_noise = model(x_noisy, t / diffusion.timesteps, params)\n",
    "                val_loss += F.mse_loss(predicted_noise, noise).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'best_val_loss': best_val_loss}, best_ckpt)\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "        torch.save({'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_val_loss': best_val_loss}, latest_ckpt)\n",
    "\n",
    "        if (epoch + 1) % vis_interval == 0:\n",
    "            print(f\"Generating visualization for epoch {epoch+1}...\")\n",
    "            visualize_progress(diffusion, val_loader, epoch + 1, device, num_samples=5)\n",
    "\n",
    "    return diffusion\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "diffusion = train_diffusion(train_loader, val_loader, epochs=200, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
