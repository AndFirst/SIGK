{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaf52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 128, 128])\n",
      "Parameters batch shape: torch.Size([32, 24])\n",
      "Min: -27.6353, Max: 41.2406, Std: 7.3882, Median: 0.0215\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.camera_pos = [5.0, 5.0, 15.0]\n",
    "        \n",
    "        self.max_r = math.sqrt(25**2 + 25**2 + 35**2)  # Max radial distance\n",
    "        self.r_range = [0, self.max_r]\n",
    "        self.theta_range = [0, math.pi]\n",
    "        self.phi_range = [-math.pi, math.pi]\n",
    "        self.shininess_range = [3, 20]\n",
    "        self.diffuse_range = [0, 1]\n",
    "        \n",
    "        self.image_files = [f\"image_{str(i).zfill(4)}.png\" for i in self.data['frame']]\n",
    "        \n",
    "        self.params_tensor = self.preprocess_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def normalize(self, value, range_min, range_max):\n",
    "        \"\"\"Normalize value to [-1, 1].\"\"\"\n",
    "        normalized = (value - range_min) / (range_max - range_min)\n",
    "        return 2 * normalized - 1\n",
    "    \n",
    "    def get_orthonormal_basis(self, z_prime):\n",
    "        \"\"\"Compute orthonormal basis with z_prime as the z'-axis.\"\"\"\n",
    "        ref = [0, 0, 1]\n",
    "        if abs(z_prime[2]) > 0.99:\n",
    "            ref = [0, 1, 0]\n",
    "        \n",
    "        x_prime = [\n",
    "            z_prime[1] * ref[2] - z_prime[2] * ref[1],\n",
    "            z_prime[2] * ref[0] - z_prime[0] * ref[2],\n",
    "            z_prime[0] * ref[1] - z_prime[1] * ref[0]\n",
    "        ]\n",
    "        x_norm = math.sqrt(sum(x * x for x in x_prime))\n",
    "        x_prime = [x / x_norm for x in x_prime]\n",
    "        \n",
    "        y_prime = [\n",
    "            z_prime[1] * x_prime[2] - z_prime[2] * x_prime[1],\n",
    "            z_prime[2] * x_prime[0] - z_prime[0] * x_prime[2],\n",
    "            z_prime[0] * x_prime[1] - z_prime[1] * x_prime[0]\n",
    "        ]\n",
    "        y_norm = math.sqrt(sum(y * y for y in y_prime))\n",
    "        y_prime = [y / y_norm for y in y_prime]\n",
    "        \n",
    "        return x_prime, y_prime, z_prime\n",
    "    \n",
    "    def cartesian_to_spherical_lookat(self, x, y, z, lookat_vec):\n",
    "        \"\"\"Convert relative Cartesian coordinates to spherical coordinates aligned with lookat vector.\"\"\"\n",
    "        x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "        \n",
    "        x_new = x * x_prime[0] + y * x_prime[1] + z * x_prime[2]\n",
    "        y_new = x * y_prime[0] + y * y_prime[1] + z * y_prime[2]\n",
    "        z_new = x * z_prime[0] + y * z_prime[1] + z * z_prime[2]\n",
    "        \n",
    "        # Convert to spherical coordinates\n",
    "        r = math.sqrt(x_new**2 + y_new**2 + z_new**2)\n",
    "        theta = math.acos(z_new / r) if r > 0 else 0\n",
    "        phi = math.atan2(y_new, x_new)\n",
    "        \n",
    "        return r, theta, phi\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess all data to compute spherical coordinates and additional relative features.\"\"\"\n",
    "        params_list = []\n",
    "\n",
    "        for idx in range(len(self.data)):\n",
    "            row = self.data.iloc[idx]\n",
    "\n",
    "            lookat_x_rel = row['lookat_target_x'] - self.camera_pos[0]\n",
    "            lookat_y_rel = row['lookat_target_y'] - self.camera_pos[1]\n",
    "            lookat_z_rel = row['lookat_target_z'] - self.camera_pos[2]\n",
    "            lookat_norm = math.sqrt(lookat_x_rel**2 + lookat_y_rel**2 + lookat_z_rel**2)\n",
    "            lookat_vec = (\n",
    "                lookat_x_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_y_rel / lookat_norm if lookat_norm > 0 else 0,\n",
    "                lookat_z_rel / lookat_norm if lookat_norm > 0 else 0\n",
    "            )\n",
    "\n",
    "            model_x_rel = row['model_translation_x'] - self.camera_pos[0]\n",
    "            model_y_rel = row['model_translation_y'] - self.camera_pos[1]\n",
    "            model_z_rel = row['model_translation_z'] - self.camera_pos[2]\n",
    "            model_r, model_theta, model_phi = self.cartesian_to_spherical_lookat(\n",
    "                model_x_rel, model_y_rel, model_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_x_rel = row['light_position_x'] - self.camera_pos[0]\n",
    "            light_y_rel = row['light_position_y'] - self.camera_pos[1]\n",
    "            light_z_rel = row['light_position_z'] - self.camera_pos[2]\n",
    "            light_r, light_theta, light_phi = self.cartesian_to_spherical_lookat(\n",
    "                light_x_rel, light_y_rel, light_z_rel, lookat_vec\n",
    "            )\n",
    "\n",
    "            light_to_model_vec = [\n",
    "                light_x_rel - model_x_rel,\n",
    "                light_y_rel - model_y_rel,\n",
    "                light_z_rel - model_z_rel\n",
    "            ]\n",
    "            ltm_norm = math.sqrt(sum(x**2 for x in light_to_model_vec))\n",
    "            light_to_model_unit = [x / ltm_norm if ltm_norm > 0 else 0 for x in light_to_model_vec]\n",
    "\n",
    "            cos_view_light = sum(l * v for l, v in zip(light_to_model_unit, lookat_vec))\n",
    "            ltm_normalized = self.normalize(ltm_norm, 0, self.max_r)\n",
    "\n",
    "            x_prime, y_prime, z_prime = self.get_orthonormal_basis(lookat_vec)\n",
    "            def rotate_to_view(x, y, z):\n",
    "                return [\n",
    "                    x * x_prime[0] + y * x_prime[1] + z * x_prime[2],\n",
    "                    x * y_prime[0] + y * y_prime[1] + z * y_prime[2],\n",
    "                    x * z_prime[0] + y * z_prime[1] + z * z_prime[2],\n",
    "                ]\n",
    "            model_view_xyz = rotate_to_view(model_x_rel, model_y_rel, model_z_rel)\n",
    "            light_view_xyz = rotate_to_view(light_x_rel, light_y_rel, light_z_rel)\n",
    "\n",
    "            params = [\n",
    "                self.normalize(model_r, *self.r_range),\n",
    "                self.normalize(model_theta, *self.theta_range),\n",
    "                self.normalize(model_phi, *self.phi_range),\n",
    "                self.normalize(light_r, *self.r_range),\n",
    "                self.normalize(light_theta, *self.theta_range),\n",
    "                self.normalize(light_phi, *self.phi_range),\n",
    "                lookat_vec[0],\n",
    "                lookat_vec[1],\n",
    "                lookat_vec[2],\n",
    "                self.normalize(row['material_diffuse_r'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_g'], *self.diffuse_range),\n",
    "                self.normalize(row['material_diffuse_b'], *self.diffuse_range),\n",
    "                self.normalize(row['material_shininess'], *self.shininess_range),\n",
    "                ltm_normalized,\n",
    "                cos_view_light,\n",
    "                *light_to_model_unit,\n",
    "                *model_view_xyz,\n",
    "                *light_view_xyz\n",
    "            ]\n",
    "\n",
    "            params_list.append(params)\n",
    "\n",
    "        return torch.tensor(params_list, dtype=torch.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        params_tensor = self.params_tensor[idx]\n",
    "        \n",
    "        return image, params_tensor\n",
    "    \n",
    "def denormalize(img):\n",
    "    \"\"\"Denormalize images from [-1, 1] to [0, 1] for visualization.\"\"\"\n",
    "    return (img * 0.5) + 0.5\n",
    "\n",
    "def get_data_loaders(csv_path, image_dir, batch_size=32, train_split=0.8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    dataset = FrameDataset(csv_path, image_dir, transform=transform)\n",
    "    \n",
    "    train_size = int(train_split * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "csv_path = \"../output/frame_parameters.csv\"\n",
    "image_dir = \"../output\"\n",
    "batch_size = 32\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(csv_path, image_dir, batch_size)\n",
    "\n",
    "for images, params in train_loader:\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Parameters batch shape: {params.shape}\")\n",
    "    \n",
    "    min_val = torch.min(params)\n",
    "    max_val = torch.max(params)\n",
    "    std_val = torch.std(params)\n",
    "    median_val = torch.median(params)\n",
    "    \n",
    "    print(f\"Min: {min_val.item():.4f}, Max: {max_val.item():.4f}, \"\n",
    "          f\"Std: {std_val.item():.4f}, Median: {median_val.item():.4f}\")\n",
    "    \n",
    "    num_samples = min(4, images.size(0))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(num_samples):\n",
    "        img = denormalize(images[i])\n",
    "        if img.ndim == 3 and img.shape[0] in [1, 3]:\n",
    "            img = img.permute(1, 2, 0)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(img.squeeze().numpy(), cmap='gray' if img.shape[-1] == 1 else None)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ae60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SingleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.t_emb_proj = nn.Linear(t_emb_dim, out_channels)\n",
    "    \n",
    "    def forward(self, x, t_emb):\n",
    "        t_emb = self.t_emb_proj(t_emb)[:, :, None, None]\n",
    "        x = self.conv(x) + t_emb\n",
    "        return x\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, in_channels, cond_dim, t_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_channels, in_channels // 2)\n",
    "        self.key = nn.Linear(cond_dim, in_channels // 2)\n",
    "        self.value = nn.Linear(cond_dim, in_channels)\n",
    "        self.scale = (in_channels // 2) ** -0.5\n",
    "        self.t_emb_proj = nn.Linear(t_emb_dim, in_channels // 2)\n",
    "    \n",
    "    def forward(self, x, cond, t_emb):\n",
    "        batch, channels, h, w = x.shape\n",
    "        x_flat = x.view(batch, channels, -1).permute(0, 2, 1)\n",
    "        q = self.query(x_flat)\n",
    "        k = self.key(cond).unsqueeze(1)\n",
    "        v = self.value(cond).unsqueeze(1)\n",
    "        t_emb = self.t_emb_proj(t_emb).unsqueeze(1)\n",
    "        q = q + t_emb\n",
    "        attn = F.softmax(q @ k.transpose(-2, -1) * self.scale, dim=-1)\n",
    "        out = (attn @ v).permute(0, 2, 1).view(batch, channels, h, w)\n",
    "        return x + out\n",
    "\n",
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_channels=128):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.enc_conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.enc_mu = nn.Conv2d(128, latent_channels, 3, padding=1)\n",
    "        self.enc_logvar = nn.Conv2d(128, latent_channels, 3, padding=1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec_up1 = nn.ConvTranspose2d(latent_channels, 128, 2, stride=2)\n",
    "        self.dec_conv1 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.dec_up2 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.dec_conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.dec_out = nn.Conv2d(32, 3, 1)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.enc_conv1(x))\n",
    "        x = self.pool(x)  # (32, 64, 64)\n",
    "        x = self.relu(self.enc_conv2(x))\n",
    "        x = self.pool(x)  # (64, 32, 32)\n",
    "        x = self.relu(self.enc_conv3(x))  # (128, 32, 32)\n",
    "        mu = self.enc_mu(x)  # (latent_channels, 32, 32)\n",
    "        logvar = self.enc_logvar(x)  # (latent_channels, 32, 32)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.relu(self.dec_up1(z))  # (128, 64, 64)\n",
    "        z = self.relu(self.dec_conv1(z))  # (64, 64, 64)\n",
    "        z = self.relu(self.dec_up2(z))  # (64, 128, 128)\n",
    "        z = self.relu(self.dec_conv2(z))  # (32, 128, 128)\n",
    "        z = self.dec_out(z)  # (3, 128, 128)\n",
    "        return torch.tanh(z)  # Output in [-1, 1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "class LatentUNet(nn.Module):\n",
    "    def __init__(self, in_channels=128, out_channels=128, cond_dim=128, t_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.inc = SingleConv(in_channels, 256, t_emb_dim)\n",
    "        \n",
    "        self.down1 = nn.MaxPool2d(2)\n",
    "        self.conv_down1 = SingleConv(256, 512, t_emb_dim)\n",
    "        \n",
    "        self.bottleneck = SingleConv(512, 1024, t_emb_dim)\n",
    "        self.attn = CrossAttention(1024, cond_dim, t_emb_dim)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.conv_up1 = SingleConv(512 + 256, 512, t_emb_dim)\n",
    "        \n",
    "        self.outc = nn.Conv2d(512, out_channels, 1)\n",
    "    \n",
    "    def forward(self, z, t, cond):\n",
    "        t_emb = self.get_timestep_embedding(t, t_emb_dim=32).to(z.device)\n",
    "        z1 = self.inc(z, t_emb)  # (256, 32, 32)\n",
    "        z2 = self.down1(z1)  # (256, 16, 16)\n",
    "        z2 = self.conv_down1(z2, t_emb)  # (512, 16, 16)\n",
    "        \n",
    "        z = self.bottleneck(z2, t_emb)  # (1024, 16, 16)\n",
    "        z = self.attn(z, cond, t_emb)\n",
    "        \n",
    "        z = self.up1(z)  # (512, 32, 32)\n",
    "        z = self.conv_up1(torch.cat([z, z1], dim=1), t_emb)  # (512, 32, 32)\n",
    "        return self.outc(z)  # (128, 32, 32)\n",
    "    \n",
    "    def get_timestep_embedding(self, t, t_emb_dim):\n",
    "        t = t.view(-1, 1)\n",
    "        half_dim = t_emb_dim // 2\n",
    "        emb = torch.exp(-torch.arange(half_dim, device=t.device).float() / half_dim * math.log(10000))\n",
    "        emb = t * emb\n",
    "        return torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "\n",
    "class CondEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=24, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim // 2, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ConditionalDiffusion(nn.Module):\n",
    "    def __init__(self, steps=512):\n",
    "        super().__init__()\n",
    "        self.vae = VariationalAutoEncoder(latent_channels=128)\n",
    "        self.cond_encoder = CondEncoder(input_dim=24, output_dim=128)\n",
    "        self.latent_unet = LatentUNet(in_channels=128, out_channels=128, cond_dim=128, t_emb_dim=32)\n",
    "        self.steps = steps\n",
    "        self.betas = self.get_beta_schedule(steps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0).to(device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def get_beta_schedule(self, T):\n",
    "        beta_start = 0.0001\n",
    "        beta_end = 0.02\n",
    "        return torch.linspace(beta_start, beta_end, T)\n",
    "    \n",
    "    def forward_diffusion(self, z0, t):\n",
    "        noise = torch.randn_like(z0)\n",
    "        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t])[:, None, None, None]\n",
    "        zt = sqrt_alpha_bar * z0 + sqrt_one_minus_alpha_bar * noise\n",
    "        return zt, noise\n",
    "    \n",
    "    def forward(self, z, t, cond):\n",
    "        conv_emb = self.cond_encoder(cond)\n",
    "        return self.latent_unet(z, t, conv_emb)\n",
    "    \n",
    "    def sample(self, params, latent_size=(128, 32, 32)):\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        z = torch.randn(1, *latent_size).to(device)\n",
    "        params = params.to(device)\n",
    "        cond_emb = self.cond_encoder(params)\n",
    "        \n",
    "        for t in reversed(range(self.steps)):\n",
    "            t_tensor = torch.full((1,), t, dtype=torch.long, device=device)\n",
    "            noise_pred = self.latent_unet(z, t_tensor, cond_emb)\n",
    "            alpha = self.alphas[t]\n",
    "            alpha_bar = self.alpha_bars[t]\n",
    "            z = (z - (1 - alpha) / torch.sqrt(1 - alpha_bar) * noise_pred) / torch.sqrt(alpha)\n",
    "            if t > 0:\n",
    "                z += torch.sqrt(self.betas[t]) * torch.randn_like(z)\n",
    "        \n",
    "        x = self.vae.decode(z)\n",
    "        return x.clamp(-1, 1)\n",
    "    \n",
    "def visualize_vae_comparison(vae, images, epoch, device, num_samples=4, output_dir='vae_visualizations'):\n",
    "    vae.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_samples = min(num_samples, len(images))\n",
    "    if num_samples == 0:\n",
    "        print(\"No samples to visualize.\")\n",
    "        return\n",
    "    \n",
    "    images = images[:num_samples].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "    \n",
    "    mse = F.mse_loss(images, recon_images, reduction='none').mean(dim=(1, 2, 3))\n",
    "    \n",
    "    real_images = (images + 1) / 2\n",
    "    recon_images = (recon_images + 1) / 2\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.reshape(num_samples, 2)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        real_img = real_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[i][0].imshow(real_img)\n",
    "        axes[i][0].set_title(f\"Real Image {i+1}\")\n",
    "        axes[i][0].axis('off')\n",
    "        \n",
    "        recon_img = recon_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[i][1].imshow(recon_img)\n",
    "        axes[i][1].set_title(f\"Reconstructed Image {i+1}\\nMSE: {mse[i].item():.4f}\")\n",
    "        axes[i][1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Real vs Reconstructed Comparison (Epoch {epoch})\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"vae_comparison_epoch_{epoch}.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved VAE visualization to {output_path}\")\n",
    "\n",
    "def pretrain_vae(vae, train_loader, val_loader, epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint_dir='vae_checkpoints'):\n",
    "    vae.to(device)\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), lr=1e-3)\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    last_checkpoint_path = os.path.join(checkpoint_dir, 'vae_last_model.pth')\n",
    "    \n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location=device)\n",
    "        vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Loaded VAE checkpoint from {last_checkpoint_path}, resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    val_iter = iter(val_loader)\n",
    "    vis_images, _ = next(val_iter)\n",
    "    vis_images = vis_images.to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        vae.train()\n",
    "        train_loss = 0\n",
    "        for images, _ in tqdm(train_loader, desc=f\"VAE Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = vae(images)\n",
    "            recon_loss = F.mse_loss(recon, images, reduction='sum') / images.size(0)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / images.size(0)\n",
    "            loss = recon_loss + 0.1 * kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        val_loss = 0\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                recon, mu, logvar = vae(images)\n",
    "                recon_loss = F.mse_loss(recon, images, reduction='sum') / images.size(0)\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / images.size(0)\n",
    "                loss = recon_loss + 0.1 * kl_loss\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        print(f\"VAE Epoch {epoch+1}, Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': vae.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, last_checkpoint_path)\n",
    "        print(f\"Saved VAE last model to {last_checkpoint_path}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, 'vae_best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': vae.state_dict(),\n",
    "                'optimization_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Saved VAE best model to {best_checkpoint_path} with Val Loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_vae_comparison(vae, vis_images, epoch + 1, device, num_samples=4)\n",
    "\n",
    "def visualize_comparison(model, images, params, epoch, device, num_samples=4, output_dir='diff_visualizations'):\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    num_samples = min(num_samples, len(images), len(params))\n",
    "    if num_samples == 0:\n",
    "        print(\"No samples to visualize.\")\n",
    "        return\n",
    "    \n",
    "    images = images[:num_samples].to(device)\n",
    "    param_samples = params[:num_samples].to(device)\n",
    "    \n",
    "    generated_images = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            gen_img = model.sample(param_samples[i:i+1], latent_size=(128, 32, 32))\n",
    "            generated_images.append(gen_img.squeeze(0))\n",
    "    \n",
    "    generated_images = torch.stack(generated_images)\n",
    "    \n",
    "    mse = F.mse_loss(images, generated_images, reduction='none').mean(dim=(1, 2, 3))\n",
    "    \n",
    "    real_images = (images + 1) / 2\n",
    "    generated_images = (generated_images + 1) / 2\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.reshape(num_samples, 2)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        real_img = real_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[i][0].imshow(real_img)\n",
    "        axes[i][0].set_title(f\"Real Image {i+1}\")\n",
    "        axes[i][0].axis('off')\n",
    "        \n",
    "        gen_img = generated_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        axes[i][1].imshow(gen_img)\n",
    "        axes[i][1].set_title(f\"Generated Image {i+1}\\nMSE: {mse[i].item():.4f}\")\n",
    "        axes[i][1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Real vs Generated Comparison (Epoch {epoch})\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"comparison_epoch_{epoch}.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to {output_path}\")\n",
    "\n",
    "def train_diffusion(model, train_loader, val_loader, epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint_dir='diff_checkpoints'):\n",
    "    model.to(device)\n",
    "    model.vae.eval()\n",
    "    optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad and not any(p is param for param in model.vae.parameters())], lr=1e-5)\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "    last_checkpoint_path = os.path.join(checkpoint_dir, 'last_model.pth')\n",
    "    \n",
    "    if os.path.exists(last_checkpoint_path):\n",
    "        checkpoint = torch.load(last_checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"Loaded checkpoint from {last_checkpoint_path}, resuming from epoch {start_epoch}\")\n",
    "    \n",
    "    val_iter = iter(val_loader)\n",
    "    vis_images, vis_params = next(val_iter)\n",
    "    vis_images, vis_params = vis_images.to(device), vis_params.to(device)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        model.vae.eval()\n",
    "        train_loss = 0\n",
    "        for images, params in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, params = images.to(device), params.to(device)\n",
    "            with torch.no_grad():\n",
    "                mu, _ = model.vae.encode(images)\n",
    "            t = torch.randint(0, len(model.betas), (images.size(0),), device=device)\n",
    "            zt, noise = model.forward_diffusion(mu, t)\n",
    "            noise_pred = model(zt, t, params)\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, params in val_loader:\n",
    "                images, params = images.to(device), params.to(device)\n",
    "                mu, _ = model.vae.encode(images)\n",
    "                t = torch.randint(0, len(model.betas), (images.size(0),), device=device)\n",
    "                zt, noise = model.forward_diffusion(mu, t)\n",
    "                noise_pred = model(zt, t, params)\n",
    "                loss = F.mse_loss(noise_pred, noise)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}\")\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "        }, last_checkpoint_path)\n",
    "        print(f\"Saved last model to {last_checkpoint_path}\")\n",
    "        \n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            best_checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }, best_checkpoint_path)\n",
    "            print(f\"Saved best model to {best_checkpoint_path} with Val Loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_comparison(model, vis_images, vis_params, epoch + 1, device, num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b17029b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded VAE checkpoint from vae_checkpoints/vae_last_model.pth, resuming from epoch 300\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(latent_channels=128)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pretrain_vae(vae, train_loader, val_loader, epochs=300, device=device, checkpoint_dir='vae_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0e1b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from diff_checkpoints/last_model.pth, resuming from epoch 135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/400: 100%|██████████| 63/63 [00:08<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136, Train Loss: 0.0573, Val Loss: 0.0533\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/400: 100%|██████████| 63/63 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137, Train Loss: 0.0523, Val Loss: 0.0704\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/400: 100%|██████████| 63/63 [00:08<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138, Train Loss: 0.0554, Val Loss: 0.0619\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/400: 100%|██████████| 63/63 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139, Train Loss: 0.0538, Val Loss: 0.0553\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/400: 100%|██████████| 63/63 [00:08<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140, Train Loss: 0.0536, Val Loss: 0.0552\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_140.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/400: 100%|██████████| 63/63 [00:08<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141, Train Loss: 0.0541, Val Loss: 0.0595\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/400: 100%|██████████| 63/63 [00:08<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142, Train Loss: 0.0579, Val Loss: 0.0443\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved best model to diff_checkpoints/best_model.pth with Val Loss: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/400: 100%|██████████| 63/63 [00:08<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143, Train Loss: 0.0561, Val Loss: 0.0578\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/400: 100%|██████████| 63/63 [00:08<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144, Train Loss: 0.0610, Val Loss: 0.0601\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/400: 100%|██████████| 63/63 [00:08<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145, Train Loss: 0.0577, Val Loss: 0.0590\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/400: 100%|██████████| 63/63 [00:08<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146, Train Loss: 0.0553, Val Loss: 0.0542\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/400: 100%|██████████| 63/63 [00:08<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147, Train Loss: 0.0511, Val Loss: 0.0636\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/400: 100%|██████████| 63/63 [00:08<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148, Train Loss: 0.0513, Val Loss: 0.0487\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/400: 100%|██████████| 63/63 [00:08<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149, Train Loss: 0.0486, Val Loss: 0.0518\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/400: 100%|██████████| 63/63 [00:08<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150, Train Loss: 0.0519, Val Loss: 0.0521\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_150.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/400: 100%|██████████| 63/63 [00:08<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151, Train Loss: 0.0497, Val Loss: 0.0589\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/400: 100%|██████████| 63/63 [00:08<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152, Train Loss: 0.0580, Val Loss: 0.0616\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/400: 100%|██████████| 63/63 [00:08<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153, Train Loss: 0.0565, Val Loss: 0.0551\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/400: 100%|██████████| 63/63 [00:08<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154, Train Loss: 0.0530, Val Loss: 0.0554\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/400: 100%|██████████| 63/63 [00:08<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155, Train Loss: 0.0527, Val Loss: 0.0573\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/400: 100%|██████████| 63/63 [00:08<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156, Train Loss: 0.0513, Val Loss: 0.0547\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/400: 100%|██████████| 63/63 [00:08<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157, Train Loss: 0.0523, Val Loss: 0.0459\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/400: 100%|██████████| 63/63 [00:08<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158, Train Loss: 0.0515, Val Loss: 0.0457\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/400: 100%|██████████| 63/63 [00:08<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159, Train Loss: 0.0557, Val Loss: 0.0540\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/400: 100%|██████████| 63/63 [00:08<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160, Train Loss: 0.0534, Val Loss: 0.0520\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_160.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/400: 100%|██████████| 63/63 [00:08<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161, Train Loss: 0.0515, Val Loss: 0.0571\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/400: 100%|██████████| 63/63 [00:08<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162, Train Loss: 0.0540, Val Loss: 0.0520\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/400: 100%|██████████| 63/63 [00:08<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163, Train Loss: 0.0466, Val Loss: 0.0477\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/400: 100%|██████████| 63/63 [00:08<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164, Train Loss: 0.0510, Val Loss: 0.0575\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/400: 100%|██████████| 63/63 [00:08<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165, Train Loss: 0.0523, Val Loss: 0.0617\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/400: 100%|██████████| 63/63 [00:08<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166, Train Loss: 0.0490, Val Loss: 0.0489\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/400: 100%|██████████| 63/63 [00:08<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167, Train Loss: 0.0546, Val Loss: 0.0576\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/400: 100%|██████████| 63/63 [00:08<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168, Train Loss: 0.0520, Val Loss: 0.0645\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/400: 100%|██████████| 63/63 [00:08<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169, Train Loss: 0.0504, Val Loss: 0.0553\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/400: 100%|██████████| 63/63 [00:08<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170, Train Loss: 0.0483, Val Loss: 0.0536\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_170.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/400: 100%|██████████| 63/63 [00:08<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171, Train Loss: 0.0562, Val Loss: 0.0519\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/400: 100%|██████████| 63/63 [00:08<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172, Train Loss: 0.0508, Val Loss: 0.0476\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/400: 100%|██████████| 63/63 [00:08<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, Train Loss: 0.0505, Val Loss: 0.0534\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/400: 100%|██████████| 63/63 [00:08<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174, Train Loss: 0.0448, Val Loss: 0.0560\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/400: 100%|██████████| 63/63 [00:09<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175, Train Loss: 0.0506, Val Loss: 0.0648\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/400: 100%|██████████| 63/63 [00:08<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176, Train Loss: 0.0526, Val Loss: 0.0585\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/400: 100%|██████████| 63/63 [00:08<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177, Train Loss: 0.0465, Val Loss: 0.0538\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/400: 100%|██████████| 63/63 [00:08<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178, Train Loss: 0.0499, Val Loss: 0.0469\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/400: 100%|██████████| 63/63 [00:08<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179, Train Loss: 0.0482, Val Loss: 0.0485\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/400: 100%|██████████| 63/63 [00:08<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180, Train Loss: 0.0517, Val Loss: 0.0448\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_180.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/400: 100%|██████████| 63/63 [00:08<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181, Train Loss: 0.0507, Val Loss: 0.0503\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/400: 100%|██████████| 63/63 [00:08<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182, Train Loss: 0.0496, Val Loss: 0.0576\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/400: 100%|██████████| 63/63 [00:08<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183, Train Loss: 0.0472, Val Loss: 0.0461\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/400: 100%|██████████| 63/63 [00:08<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184, Train Loss: 0.0526, Val Loss: 0.0497\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/400: 100%|██████████| 63/63 [00:08<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185, Train Loss: 0.0485, Val Loss: 0.0384\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved best model to diff_checkpoints/best_model.pth with Val Loss: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/400: 100%|██████████| 63/63 [00:08<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186, Train Loss: 0.0452, Val Loss: 0.0528\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/400: 100%|██████████| 63/63 [00:08<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187, Train Loss: 0.0508, Val Loss: 0.0570\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/400: 100%|██████████| 63/63 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, Train Loss: 0.0497, Val Loss: 0.0417\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/400: 100%|██████████| 63/63 [00:09<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189, Train Loss: 0.0481, Val Loss: 0.0481\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/400: 100%|██████████| 63/63 [00:08<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Train Loss: 0.0451, Val Loss: 0.0451\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_190.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/400: 100%|██████████| 63/63 [00:08<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191, Train Loss: 0.0571, Val Loss: 0.0475\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/400: 100%|██████████| 63/63 [00:08<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, Train Loss: 0.0477, Val Loss: 0.0509\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193/400: 100%|██████████| 63/63 [00:08<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193, Train Loss: 0.0514, Val Loss: 0.0543\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194/400: 100%|██████████| 63/63 [00:09<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194, Train Loss: 0.0486, Val Loss: 0.0485\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195/400: 100%|██████████| 63/63 [00:08<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, Train Loss: 0.0507, Val Loss: 0.0533\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/400: 100%|██████████| 63/63 [00:08<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196, Train Loss: 0.0524, Val Loss: 0.0478\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/400: 100%|██████████| 63/63 [00:08<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197, Train Loss: 0.0486, Val Loss: 0.0459\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198/400: 100%|██████████| 63/63 [00:08<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198, Train Loss: 0.0558, Val Loss: 0.0498\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199/400: 100%|██████████| 63/63 [00:08<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199, Train Loss: 0.0498, Val Loss: 0.0586\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200/400: 100%|██████████| 63/63 [00:08<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Train Loss: 0.0491, Val Loss: 0.0557\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_200.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201/400: 100%|██████████| 63/63 [00:08<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201, Train Loss: 0.0481, Val Loss: 0.0509\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202/400: 100%|██████████| 63/63 [00:08<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202, Train Loss: 0.0471, Val Loss: 0.0385\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 203/400: 100%|██████████| 63/63 [00:08<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203, Train Loss: 0.0435, Val Loss: 0.0448\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 204/400: 100%|██████████| 63/63 [00:08<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204, Train Loss: 0.0430, Val Loss: 0.0405\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205/400: 100%|██████████| 63/63 [00:09<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205, Train Loss: 0.0483, Val Loss: 0.0551\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 206/400: 100%|██████████| 63/63 [00:08<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206, Train Loss: 0.0526, Val Loss: 0.0521\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 207/400: 100%|██████████| 63/63 [00:09<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207, Train Loss: 0.0497, Val Loss: 0.0479\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 208/400: 100%|██████████| 63/63 [00:08<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208, Train Loss: 0.0496, Val Loss: 0.0466\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 209/400: 100%|██████████| 63/63 [00:08<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209, Train Loss: 0.0423, Val Loss: 0.0452\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 210/400: 100%|██████████| 63/63 [00:08<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210, Train Loss: 0.0490, Val Loss: 0.0433\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_210.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 211/400: 100%|██████████| 63/63 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211, Train Loss: 0.0473, Val Loss: 0.0510\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212/400: 100%|██████████| 63/63 [00:08<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212, Train Loss: 0.0486, Val Loss: 0.0567\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 213/400: 100%|██████████| 63/63 [00:09<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213, Train Loss: 0.0446, Val Loss: 0.0482\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 214/400: 100%|██████████| 63/63 [00:08<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214, Train Loss: 0.0472, Val Loss: 0.0362\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved best model to diff_checkpoints/best_model.pth with Val Loss: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 215/400: 100%|██████████| 63/63 [00:08<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215, Train Loss: 0.0486, Val Loss: 0.0521\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 216/400: 100%|██████████| 63/63 [00:08<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216, Train Loss: 0.0504, Val Loss: 0.0401\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 217/400: 100%|██████████| 63/63 [00:08<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217, Train Loss: 0.0459, Val Loss: 0.0497\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 218/400: 100%|██████████| 63/63 [00:09<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218, Train Loss: 0.0448, Val Loss: 0.0428\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 219/400: 100%|██████████| 63/63 [00:08<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219, Train Loss: 0.0507, Val Loss: 0.0475\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 220/400: 100%|██████████| 63/63 [00:08<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220, Train Loss: 0.0442, Val Loss: 0.0432\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved visualization to diff_visualizations/comparison_epoch_220.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 221/400: 100%|██████████| 63/63 [00:08<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221, Train Loss: 0.0509, Val Loss: 0.0497\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222/400: 100%|██████████| 63/63 [00:08<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222, Train Loss: 0.0452, Val Loss: 0.0526\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 223/400: 100%|██████████| 63/63 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223, Train Loss: 0.0427, Val Loss: 0.0538\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 224/400: 100%|██████████| 63/63 [00:08<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224, Train Loss: 0.0456, Val Loss: 0.0416\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225/400: 100%|██████████| 63/63 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225, Train Loss: 0.0443, Val Loss: 0.0432\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 226/400: 100%|██████████| 63/63 [00:08<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226, Train Loss: 0.0458, Val Loss: 0.0599\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 227/400: 100%|██████████| 63/63 [00:08<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227, Train Loss: 0.0482, Val Loss: 0.0399\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 228/400: 100%|██████████| 63/63 [00:08<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228, Train Loss: 0.0427, Val Loss: 0.0437\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 229/400: 100%|██████████| 63/63 [00:08<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229, Train Loss: 0.0495, Val Loss: 0.0465\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 230/400: 100%|██████████| 63/63 [00:08<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230, Train Loss: 0.0445, Val Loss: 0.0334\n",
      "Saved last model to diff_checkpoints/last_model.pth\n",
      "Saved best model to diff_checkpoints/best_model.pth with Val Loss: 0.0334\n",
      "Saved visualization to diff_visualizations/comparison_epoch_230.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 231/400: 100%|██████████| 63/63 [00:08<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231, Train Loss: 0.0427, Val Loss: 0.0454\n",
      "Saved last model to diff_checkpoints/last_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 232/400:  38%|███▊      | 24/63 [00:03<00:05,  6.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      3\u001b[39m model.vae.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33mvae_checkpoints/vae_best_model.pth\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 388\u001b[39m, in \u001b[36mtrain_diffusion\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, device, checkpoint_dir)\u001b[39m\n\u001b[32m    386\u001b[39m     optimizer.zero_grad()\n\u001b[32m    387\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m     train_loss += loss.item()\n\u001b[32m    391\u001b[39m val_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/adam.py:739\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    736\u001b[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    738\u001b[39m     bias_correction1 = [\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m         \u001b[32m1\u001b[39m - beta1 ** \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    740\u001b[39m     ]\n\u001b[32m    741\u001b[39m     bias_correction2 = [\n\u001b[32m    742\u001b[39m         \u001b[32m1\u001b[39m - beta2 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    743\u001b[39m     ]\n\u001b[32m    745\u001b[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dokumenty/GitHub/SIGK/04_rendering_neuralny/venv/lib/python3.13/site-packages/torch/optim/optimizer.py:94\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = ConditionalDiffusion(steps=512)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.vae.load_state_dict(torch.load('vae_checkpoints/vae_best_model.pth')['model_state_dict'])\n",
    "train_diffusion(model, train_loader, val_loader, epochs=400, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
